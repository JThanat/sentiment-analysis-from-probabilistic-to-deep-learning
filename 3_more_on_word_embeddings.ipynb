{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More On Word Embeddings\n",
    "\n",
    "- effect of epoch\n",
    "- efect of windows\n",
    "- effect of dim\n",
    "\n",
    "\n",
    "\n",
    "class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (word_emb.py, line 110)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/pmeemeng/miniconda/envs/procore-data/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-17a1c441f87d>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from lib.word_emb import run_pipeline, train_or_load_wv, train_or_load_wv_transfer\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/pmeemeng/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/word_emb.py\"\u001b[0;36m, line \u001b[0;32m110\u001b[0m\n\u001b[0;31m    n_transfer = len(word2vec_vocab) if not n_transfer\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from lib.dataset import download_tfds_imdb_as_text, download_tfds_imdb_as_text_tiny\n",
    "from lib.word_emb import run_pipeline, train_or_load_wv, train_or_load_wv_transfer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = download_tfds_imdb_as_text() # tuple of (X_train, X_test, y_train, y_test)\n",
    "tiny_dataset = download_tfds_imdb_as_text_tiny() # first 100 samples from dataset\n",
    "\n",
    "PRETRAINED_WV_MODEL_PATH = \"./GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dafault\n",
    "\n",
    "dim 300\n",
    "windows 5\n",
    "iter 5\n",
    "skip gram\n",
    "minvount 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.87 2\n",
      "F1 on test set: 0.86\n"
     ]
    }
   ],
   "source": [
    "model_train = train_or_load_wv(dataset[0]) \n",
    "_, vectorizer = run_pipeline(dataset, model_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Training Word2Vec...\n",
    "Loading tokenized document from disk...\n",
    "Finished loading tokenized document in 0.37s!\n",
    "Finished training Word2Vec and saved to disk!\n",
    "Loading tokenized document from disk...\n",
    "Finished loading tokenized document in 0.37s!\n",
    "Loading tokenized document from disk...\n",
    "Finished loading tokenized document in 0.36s!\n",
    "Best parameters set found on development set:  {'C': 1000}\n",
    "Best F1 on development set: 0.87\n",
    "F1 on test set: 0.86\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windows\n",
    "\n",
    "In this experiment, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Word2Vec from disk!\n",
      "Loading tokenized document from disk...\n",
      "Finished loading tokenized document in 0.37s!\n",
      "Loading tokenized document from disk...\n",
      "Finished loading tokenized document in 0.35s!\n",
      "Best parameters set found on development set:  {'C': 100}\n",
      "Best F1 on development set: 0.89\n",
      "F1 on test set: 0.88\n",
      "Load Word2Vec from disk!\n",
      "Loading tokenized document from disk...\n",
      "Finished loading tokenized document in 0.37s!\n",
      "Loading tokenized document from disk...\n",
      "Finished loading tokenized document in 0.36s!\n",
      "Best parameters set found on development set:  {'C': 10}\n",
      "Best F1 on development set: 0.89\n",
      "F1 on test set: 0.88\n",
      "681.8944387435913\n"
     ]
    }
   ],
   "source": [
    "# approximate running time: 12 mins\n",
    "\n",
    "model_train_window_15 = train_or_load_wv(dataset[0],window=15)\n",
    "_, _ = run_pipeline(dataset, model_train_window_15)\n",
    "\n",
    "model_train_window_30 = train_or_load_wv(dataset[0], window=30)\n",
    "_, _ = run_pipeline(dataset, model_train_window_30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Load Word2Vec from disk!\n",
    "Loading tokenized document from disk...\n",
    "Finished loading tokenized document in 0.37s!\n",
    "Loading tokenized document from disk...\n",
    "Finished loading tokenized document in 0.35s!\n",
    "Best parameters set found on development set:  {'C': 100}\n",
    "Best F1 on development set: 0.89\n",
    "F1 on test set: 0.88\n",
    "Load Word2Vec from disk!\n",
    "Loading tokenized document from disk...\n",
    "Finished loading tokenized document in 0.37s!\n",
    "Loading tokenized document from disk...\n",
    "Finished loading tokenized document in 0.36s!\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.89\n",
    "F1 on test set: 0.88\n",
    "681.8944387435913\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.88 2\n",
      "F1 on test set: 0.87\n",
      "Best parameters set found on development set:  {'C': 100}\n",
      "Best F1 on development set: 0.89 2\n",
      "F1 on test set: 0.87\n",
      "Best parameters set found on development set:  {'C': 10}\n",
      "Best F1 on development set: 0.89 2\n",
      "F1 on test set: 0.87\n",
      "3352.4832718372345\n"
     ]
    }
   ],
   "source": [
    "# approximate running time: 40 mins\n",
    "import time\n",
    "\n",
    "now = time.time()\n",
    "\n",
    "model_train_iter_2 = train_or_load_wv(dataset[0], iter=2, window=15)\n",
    "_, _ = run_pipeline(dataset, model_train_iter_2)\n",
    "\n",
    "model_train_iter_10 = train_or_load_wv(dataset[0],iter=10, window=15)\n",
    "_, _ = run_pipeline(dataset, model_train_iter_10)\n",
    "\n",
    "model_train_iter_15 = train_or_load_wv(dataset[0], iter=30, window=15)\n",
    "_, _ = run_pipeline(dataset, model_train_iter_15)\n",
    "\n",
    "\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Best parameters set found on development set:  {'C': 1000}\n",
    "Best F1 on development set: 0.88 2\n",
    "F1 on test set: 0.87\n",
    "Best parameters set found on development set:  {'C': 100}\n",
    "Best F1 on development set: 0.89 2\n",
    "F1 on test set: 0.87\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.89 2\n",
    "F1 on test set: 0.87\n",
    "3352.4832718372345\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:  {'C': 100}\n",
      "Best F1 on development set: 0.88\n",
      "F1 on test set: 0.87\n",
      "Best parameters set found on development set:  {'C': 100}\n",
      "Best F1 on development set: 0.89\n",
      "F1 on test set: 0.88\n",
      "Best parameters set found on development set:  {'C': 100}\n",
      "Best F1 on development set: 0.89\n",
      "F1 on test set: 0.88\n",
      "3940.248155117035\n"
     ]
    }
   ],
   "source": [
    "# approximate running time: 40 mins\n",
    "import time\n",
    "\n",
    "now = time.time()\n",
    "\n",
    "model_train_dim_100 = train_or_load_wv(dataset[0], size=100, window=15)\n",
    "_, _ = run_pipeline(dataset, model_train_dim_100)\n",
    "\n",
    "model_train_dim_500 = train_or_load_wv(dataset[0], size=500, window=15)\n",
    "_, _ = run_pipeline(dataset, model_train_dim_500)\n",
    "\n",
    "model_train_dim_1000 = train_or_load_wv(dataset[0], size=1000, window=15)\n",
    "_, _ = run_pipeline(dataset, model_train_dim_1000)\n",
    "\n",
    "\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer\n",
    "- lock\n",
    "- not lock\n",
    "- limit vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "model_transfer_lock = train_or_load_wv_transfer(dataset[0], PRETRAINED_WV_MODEL_PATH)\n",
    "_, _ = run_pipeline(dataset, model_transfer_lock)\n",
    "\n",
    "model_transfer_not_lock = train_or_load_wv_transfer(dataset\n",
    "                                                    [0], PRETRAINED_WV_MODEL_PATH, lockf=1)\n",
    "_, _ = run_pipeline(dataset, model_transfer_not_lock)\n",
    "\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Best parameters set found on development set:  {'C': 1000}\n",
    "Best F1 on development set: 0.85 2\n",
    "F1 on test set: 0.85\n",
    "Best parameters set found on development set:  {'C': 1000}\n",
    "Best F1 on development set: 0.85 2\n",
    "F1 on test set: 0.85\n",
    "3211.6761479377747\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.86\n",
      "F1 on test set: 0.85\n",
      "fit_transform\n",
      "oov freq 0.0\n",
      "%unk in vocab 0.0\n",
      "transform\n",
      "oov freq 0.005682488993566625\n",
      "%unk in vocab 0.010098350178710915\n",
      "2014.812409877777\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "model_transfer_lock = train_or_load_wv_transfer(dataset[0], PRETRAINED_WV_MODEL_PATH, iter=2, window=15)\n",
    "_, dense_lock = run_pipeline(dataset, model_transfer_lock)\n",
    "dense_lock.print_stat()\n",
    "# model_transfer_not_lock = train_or_load_wv_transfer(dataset[0], PRETRAINED_WV_MODEL_PATH, lockf=1, iter=2)\n",
    "# _, dense_unlock = run_pipeline(dataset, model_transfer_not_lock)\n",
    "\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.86\n",
      "F1 on test set: 0.85\n",
      "fit_transform\n",
      "oov freq 0.0\n",
      "%unk in vocab 0.0\n",
      "transform\n",
      "oov freq 0.005682488993566625\n",
      "%unk in vocab 0.010098350178710915\n",
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.86\n",
      "F1 on test set: 0.85\n",
      "2545.27494764328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "model_transfer_lock = train_or_load_wv_transfer(dataset[0], PRETRAINED_WV_MODEL_PATH, iter=5, window=15)\n",
    "_, dense_lock = run_pipeline(dataset, model_transfer_lock)\n",
    "dense_lock.print_stat()\n",
    "model_transfer_not_lock = train_or_load_wv_transfer(dataset[0], PRETRAINED_WV_MODEL_PATH, lockf=1, iter=5,window=15)\n",
    "_, dense_unlock = run_pipeline(dataset, model_transfer_not_lock)\n",
    "\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_transform\n",
      "oov freq 0.0\n",
      "%unk in vocab 0.0\n",
      "transform\n",
      "oov freq 0.005682488993566625\n",
      "%unk in vocab 0.010098350178710915\n"
     ]
    }
   ],
   "source": [
    "dense_unlock.print_stat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrained_word2vec = KeyedVectors.load_word2vec_format(PRETRAINED_WV_MODEL_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pretrained_word2vec.vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "model_transfer_n_transfer_100k = train_or_load_wv_transfer(\n",
    "    dataset[0], \n",
    "    PRETRAINED_WV_MODEL_PATH, \n",
    "    lockf=1, \n",
    "    iter=5,\n",
    "    window=15,\n",
    "    n_transfer=100000\n",
    ")\n",
    "_, dense_unlock = run_pipeline(dataset, model_transfer_n_transfer_100k)\n",
    "\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
