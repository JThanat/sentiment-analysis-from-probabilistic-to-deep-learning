{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import download_tfds_imdb_as_text\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nlp_utils import spacy_tokenizer_lower_lemma_remove_stop, preprocess_remove_html_non_ascii, spacy_tokenizer,spacy_tokenizer_remove_stop\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples 25000\n",
      "number of testing samples 25000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = download_tfds_imdb_as_text()\n",
    "X_train_preprocessed = [preprocess_remove_html_non_ascii(doc) for doc in X_train]\n",
    "X_train_tokenized = [[e for e in nlp(doc, disable=[\"tagger\", \"parser\"])] for doc in X_train_preprocessed]\n",
    "\n",
    "X_train_tokenized_v2 = [nlp(doc) for doc in X_train_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spacy(spacy_tok):\n",
    "    return not (\n",
    "        spacy_tok.is_bracket or\n",
    "        spacy_tok.is_quote or\n",
    "        not spacy_tok.is_ascii or\n",
    "        spacy_tok.is_currency or\n",
    "        spacy_tok.is_digit or\n",
    "        spacy_tok.is_space or\n",
    "        spacy_tok.like_email or\n",
    "        spacy_tok.like_num or\n",
    "        spacy_tok.like_url\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_spacy2(spacy_tok):\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_filtered = [[e.orth_ for e in filter( filter_spacy, doc)] for doc in X_train_tokenized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter([e for s in X_train_tokenized_filtered for e in s])\n",
    "imdb_vocab = set(k for k,v in counter.items() if v >1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower + imdb\n",
    "\n",
    "vocab_1 = imdb_vocab.union(set( list(model_word2vec.vocab)[:300000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309387"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"followable\" in vocab_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec(size=300, window=7, min_count=1, workers=4, sg=1)\n",
    "model.build_vocab([list(vocab_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intersect_word2vec_format(fname='./GoogleNews-vectors-negative300.bin',\n",
    "                               lockf=0,\n",
    "                               binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"riemann\" in model_word2vec)\n",
    "print(\"Dishum\" in model_word2vec)\n",
    "print(\"followable\" in model_word2vec)\n",
    "\n",
    "\n",
    "print(\"riemann\" in model.wv)\n",
    "print(\"Dishum\" in model.wv)\n",
    "print(\"followable\" in model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"cat\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00020234,  0.00144157,  0.00119393, -0.0010093 ,  0.00090943,\n",
       "        0.00082218, -0.00018011,  0.00114797,  0.00020639, -0.00142701],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"Dishum\"][:10] # look like random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00011482, -0.00020896, -0.00050605, -0.00028058, -0.00147494,\n",
       "       -0.00062779, -0.00077607, -0.00078202,  0.00148383, -0.00016087],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"followable\"][:10] # look like random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6536044, 6564848)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(X_train_tokenized_filtered_v2, total_examples=len(X_train_tokenized_filtered_v2), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05079229,  0.03941049,  0.02097265,  0.07971535, -0.05137871,\n",
       "       -0.0041139 ,  0.03097997, -0.07551193,  0.0709124 ,  0.06988747],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"followable\"][:10] # after train  2 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"cat\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model.tf.300k.1.case.v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(X_train_tokenized[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This was an absolutely terrible movie.,\n",
       " Don't be lured in by Christopher Walken or Michael Ironside.,\n",
       " Both are great actors, but this must simply be their worst role in history.,\n",
       " Even their great acting could not redeem this movie's ridiculous storyline.,\n",
       " This movie is an early nineties US propaganda piece.,\n",
       " The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions.,\n",
       " Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning.,\n",
       " I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name.,\n",
       " I could barely sit through it.]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in X_train_tokenized_v2[0].sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_v2 = [nlp(doc) for doc in X_train_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenized_corpus = [s for instance in X_train_tokenized_v2 for s in instance.sents ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_filtered_v2 = [[e.orth_ for e in filter( filter_spacy, doc)] for doc in sentenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_filtered_v2_no_fulstop = [list(filter(lambda x: x!=\".\", s)) for s in X_train_tokenized_filtered_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(X_train_tokenized_filtered_v2, size=240, window=5, min_count=1, workers=4, sg=1, iter=10)\n",
    "model.save(\"word2vec.model.240.5.10.filtered.v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'lured',\n",
       " 'in',\n",
       " 'by',\n",
       " 'Christopher',\n",
       " 'Walken',\n",
       " 'or',\n",
       " 'Michael',\n",
       " 'Ironside']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized_filtered_v2_no_fulstop[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainThread\n",
      "Thread-2\n",
      "Thread-3\n",
      "IPythonHistorySavingThread\n",
      "Thread-1\n",
      "Thread-4\n",
      "Thread-215\n",
      "Thread-216\n",
      "Thread-217\n",
      "Thread-218\n",
      "Thread-219\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "for thread in threading.enumerate():\n",
    "    print(thread.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = [e.orth_.lower() for sent in X_train_tokenized for e in sent]\n",
    "X_train, X_test, y_train, y_test = download_tfds_imdb_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6797828"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5/31 start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import download_tfds_imdb_as_text\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pickle\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from wvtf2 import MyWord2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = download_tfds_imdb_as_text()\n",
    "# corpus = list()\n",
    "\n",
    "# for s in nlp.pipe(X_train, disable=[\"tagger\", \"parser\"], n_threads=4):\n",
    "#     for tok in s:\n",
    "#         if tok.is_digit or tok.is_punct or tok.like_url or tok.like_num or tok.like_email:\n",
    "#             continue\n",
    "#         corpus.append(tok.orth_.lower())\n",
    "\n",
    "# pickle.dump(corpus, open(\"corpus-2020-05-31.pkl\", \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =  pickle.load( open(\"corpus-2020-05-31.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'was',\n",
       " 'an',\n",
       " 'absolutely',\n",
       " 'terrible',\n",
       " 'movie',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'lured']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count: 5944282\n",
      "Unique words: 100161\n",
      "Vocabulary size: 100000\n",
      "Most common words: [('UNK', 162), ('the', 328822), ('and', 162887), ('a', 161730), ('of', 145595), ('to', 135355), ('is', 110135), ('it', 93487), ('in', 92823), ('i', 82706)]\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 300\n",
    "max_vocabulary_size = 100000\n",
    "min_occurrence = 1\n",
    "skip_window = 10\n",
    "batch_size = 1000\n",
    "epoch = 50\n",
    "\n",
    "\n",
    "myWV = MyWord2Vec(corpus, embedding_size, max_vocabulary_size, min_occurrence, skip_window, batch_size, epoch, num_skips=2, num_sampled=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 126.508642442594\n",
      "epoch 1 loss 52.73447093952393\n",
      "epoch 2 loss 34.810731032887546\n",
      "epoch 3 loss 26.856889772058206\n",
      "epoch 4 loss 22.242692825300697\n",
      "epoch 5 loss 19.37683467911515\n",
      "epoch 6 loss 17.347419358230297\n",
      "epoch 7 loss 15.849497171755404\n",
      "epoch 8 loss 14.703792633947346\n",
      "epoch 9 loss 13.774224072672219\n",
      "epoch 10 loss 13.003493250063084\n",
      "epoch 11 loss 12.3770410148036\n",
      "epoch 12 loss 11.824668548658424\n",
      "epoch 13 loss 11.389033455294811\n",
      "epoch 14 loss 10.982524550004205\n",
      "epoch 15 loss 10.625477726259568\n",
      "epoch 16 loss 10.303933783329128\n",
      "epoch 17 loss 10.019241131508117\n"
     ]
    }
   ],
   "source": [
    "emb, idx = myWV.train()\n",
    "\n",
    "wv = dict()\n",
    "for i, vocab in idx.items():\n",
    "    wv[vocab] = emb[i,:]\n",
    "    \n",
    "pickle.dump(wv, open(\"wv_20200531-remove-punct-digit.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
