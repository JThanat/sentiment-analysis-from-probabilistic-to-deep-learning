{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Prerequisite - If you are not familiar with word embeddings, read [this](https://web.stanford.edu/~jurafsky/slp3/6.pdf).\n",
    "\n",
    "Study\n",
    "- model token effect\n",
    "- polling technique effect\n",
    "- model changes\n",
    "    - GloVe\n",
    "    - Word2Vec\n",
    "    - train on train data \n",
    "    - transfer learning\n",
    "- for train on train data\n",
    "    - effect of span\n",
    "    - effect of dim\n",
    "    - effect of epoch (can it overfit?)\n",
    "\n",
    "- deal with unknwon\n",
    " - zero\n",
    " - random\n",
    " - average\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from lib.dataset import download_tfds_imdb_as_text, download_tfds_imdb_as_text_tiny\n",
    "from lib.word_emb import run_pipeline\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_models = {\n",
    "    \"word2vec\": gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True),\n",
    "    \"glove\": gensim.models.KeyedVectors.load_word2vec_format('./glove.840B.300d.w2vformat.txt', binary=False) \n",
    "}\n",
    "\n",
    "dataset  = download_tfds_imdb_as_text()\n",
    "tiny_dataset = download_tfds_imdb_as_text_tiny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 - Effect text preprocess\n",
    "\n",
    "In this experiment, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp1(dataset):\n",
    "    \n",
    "    print(\"Simple SpaCy tokenizer\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"])\n",
    "\n",
    "    print(\"Simple SpaCy tokenizer and lowercase\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], lower=True)\n",
    "    \n",
    "    print(\"Simple SpaCy tokenizer, lowercase, ignore stop words and numbers\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], lower=True, ignore=[\"like_num\", \"is_stop\"])\n",
    "\n",
    "# approximate running time: 16 mins\n",
    "exp1(dataset)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1 Discussion**\n",
    "```\n",
    "Simple SpaCy tokenizer\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "Simple SpaCy tokenizer and lowercase\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "Simple SpaCy tokenizer, lowercase, ignore stop words and numbers\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.84\n",
    "F1 on test set: 0.84\n",
    "954.2723577022552\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 - Embeddings\n",
    "\n",
    "In this experiment, we will use two different word embeddings, [Word2Vec](https://arxiv.org/pdf/1310.4546.pdf) and [GloVE](https://nlp.stanford.edu/projects/glove/). The high level intuitions of both embeddings are similar in the sense that they both estimate dense representation of words based on co-occurrence, i.e. words that are replaceable are similar. However, their models are very different. In a nutshell, GloVE directly estimates embeddings from co-occurrence matrix, while Word2Vec is a learning based model that learns to predict neighboring words from center words (skip-gram) or other way around (C-BOW). More info, see [this](https://www.quora.com/How-is-GloVe-different-from-word2vec).\n",
    "\n",
    "We will use pre-trained Word2Vec and GloVE. The pre-trained Word2Vec has 3M words, trained on roughly 100B tokens from a Google News dataset. The vector length is 300 features. More info, see [this](https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/). The pre-trained GloVE model has 2.2M words, trained on 840B tokens from Common Crawl. The vector length is also 300 features. In sum\n",
    "- both trained on very large corpus (100B vs 840B)\n",
    "- both trained on general corpus (Google News vs Common Crawl)\n",
    "- both has 300 features\n",
    "\n",
    "\n",
    "Also note that differences of embeddings in this experiment is not only the models (Word2Vec vs GloVE) but also the data they were trained. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp2(dataset):\n",
    "    print(\"Word2Vec\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"])\n",
    "\n",
    "    print(\"GloVe\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"glove\"])\n",
    "    \n",
    "\n",
    "# approximate running time: 13 mins\n",
    "import time\n",
    "now = time.time()\n",
    "exp2(dataset)\n",
    "print(time.time()-now)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2 Discussion**\n",
    "\n",
    "\n",
    "```\n",
    "Word2Vec\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "GloVe\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 1000}\n",
    "Best F1 on development set: 0.86\n",
    "F1 on test set: 0.85\n",
    "742.7005350589752\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 - of tfidf\n",
    "\n",
    "In this experiment,...\n",
    "\n",
    "useful for IR but may not for classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp3(dataset):\n",
    "    print(\"norm\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"])\n",
    "    \n",
    "    print(\"norm + idf\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], tfidf=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# approximate running time: 12 mins\n",
    "import time\n",
    "now = time.time()\n",
    "exp3(dataset)\n",
    "print(time.time()-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "```\n",
    "norm\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "norm + idf\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 100}\n",
    "Best F1 on development set: 0.83\n",
    "F1 on test set: 0.83\n",
    "677.7894566059113\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 - of polling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp4(dataset):\n",
    "    print(\"norm\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], polling=\"norm\")\n",
    "    \n",
    "    print(\"sum\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], polling=\"sum\")\n",
    "    \n",
    "    print(\"log\")\n",
    "    _, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], polling=\"log\")\n",
    "    \n",
    "# approximate running time: 60 mins\n",
    "import time\n",
    "now = time.time()\n",
    "exp4(dataset)\n",
    "print(time.time()-now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Exp1-4**\n",
    "\n",
    "```\n",
    "\n",
    "norm\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 100}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "sum\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 0.001}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "log\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 0.001}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.84\n",
    "3642.0309517383575\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- generalize, not domain specific\n",
    "- too many mssing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
