{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "In Notebook 1, we represent a document with a vector whose size is equal to the the size of vocabulary set. We encoded each words with one hot encoding technique which assign a value in the vector at index corresponding to the vocabulary and leave other elements zero. This technique has several drawbacks. It creates sparseness in vector space. It also cannot capture two different words that are synonym or similar, or share some sort of relations. For example, word `cat` and `dog` will totally different, as different as words like `electrical` and `poem`. These weaknesses can undermine downstream tasks. To solve these issues, researcher comes up with [dense representation](https://web.stanford.edu/~jurafsky/slp3/6.pdf) to be contrast with the sparseness of one hot encoding. Several approaches on dense representation have been studies since 1990s and culminated at the invention of [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) in 2013. The technique (seemingly, since it's still controversial) outperforms previous dense representation techniques discovered in 1990s for many downstream tasks. Our experiment will be center on pre-trained [Word2Vec] by exploring different ways of using it and their performances.\n",
    "\n",
    "    \n",
    "**Prerequisite**\n",
    "\n",
    "1. Download [Google Word2Vec Model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) to this directory (3M vocab, cased, 300 d) and run \n",
    "\n",
    "    ```\n",
    "    gunzip GoogleNews-vectors-negative300.bin.gz\n",
    "    ```\n",
    "\n",
    "2. Download [Stanford GloVe Model](http://nlp.stanford.edu/data/glove.840B.300d.zip) (2.2M vocab, cased, 300d) to this directory and run the following commands.\n",
    "\n",
    "    ```\n",
    "    unzip glove.840B.300d.zip\n",
    "    python -m gensim.scripts.glove2word2vec --input glove.840B.300d.txt --output glove.840B.300d.w2vformat.txt\n",
    "    ```\n",
    "\n",
    "GloVe is also available in SpaCy's `en_core_web_md` too. See [Document](https://spacy.io/models/en#en_core_web_md). In this notebook, we will not use GloVe from SpaCy due to lots of its limitations.\n",
    "\n",
    "If you already have those files or you don't want to save it in this directory, you can either change constant variable PRETRAINED_WV_MODEL_PATH  and PRETRAINED_GLOVE_MODEL_PATH or create symbolic link.\n",
    "    \n",
    "```\n",
    "ln -s /path/to/your/word2vec ./GoogleNews-vectors-negative300.bin\n",
    "ln -s /path/to/your/glove ./glove.840B.300d.w2vformat.txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from lib.dataset import download_tfds_imdb_as_text, download_tfds_imdb_as_text_tiny\n",
    "from lib.word_emb import run_pipeline\n",
    "import gensim\n",
    "\n",
    "PRETRAINED_WV_MODEL_PATH = \"./GoogleNews-vectors-negative300.bin\"\n",
    "PRETRAINED_GLOVE_MODEL_PATH = \"./glove.840B.300d.w2vformat.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_models = {\n",
    "    \"word2vec\": gensim.models.KeyedVectors.load_word2vec_format(PRETRAINED_WV_MODEL_PATH, binary=True),\n",
    "    \"glove\": gensim.models.KeyedVectors.load_word2vec_format(PRETRAINED_GLOVE_MODEL_PATH, binary=False) \n",
    "}\n",
    "\n",
    "dataset  = download_tfds_imdb_as_text()\n",
    "tiny_dataset = download_tfds_imdb_as_text_tiny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 - Effect text preprocess\n",
    "\n",
    "In this experiment, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple SpaCy tokenizer\n",
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 318.11\n",
      "Simple SpaCy tokenizer and lowercase\n",
      "Best parameters set found on development set:  {'C': 10}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 318.46\n",
      "Simple SpaCy tokenizer, lowercase, ignore stop words and numbers\n",
      "Best parameters set found on development set:  {'C': 10}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.84\n",
      "time: 277.41\n"
     ]
    }
   ],
   "source": [
    "# approximate running time: 16 mins\n",
    "    \n",
    "print(\"Simple SpaCy tokenizer\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"])\n",
    "\n",
    "print(\"Simple SpaCy tokenizer and lowercase\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], lower=True)\n",
    "\n",
    "print(\"Simple SpaCy tokenizer, lowercase, ignore stop words and numbers\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], lower=True, ignore=[\"like_num\", \"is_stop\"])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1 Discussion**\n",
    "```\n",
    "Simple SpaCy tokenizer\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "Simple SpaCy tokenizer and lowercase\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "Simple SpaCy tokenizer, lowercase, ignore stop words and numbers\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.84\n",
    "F1 on test set: 0.84\n",
    "954.2723577022552\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 - Embeddings\n",
    "\n",
    "In this experiment, we will use two different word embeddings, [Word2Vec](https://arxiv.org/pdf/1310.4546.pdf) and [GloVE](https://nlp.stanford.edu/projects/glove/). The high level intuitions of both embeddings are similar in the sense that they both estimate dense representation of words based on co-occurrence, i.e. words that are replaceable are similar. However, their models are very different. In a nutshell, GloVE directly estimates embeddings from co-occurrence matrix, while Word2Vec is a learning based model that learns to predict neighboring words from center words (skip-gram) or other way around (C-BOW). More info, see [this](https://www.quora.com/How-is-GloVe-different-from-word2vec).\n",
    "\n",
    "We will use pre-trained Word2Vec and GloVE. The pre-trained Word2Vec has 3M words, trained on roughly 100B tokens from a Google News dataset. The vector length is 300 features. More info, see [this](https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/). The pre-trained GloVE model has 2.2M words, trained on 840B tokens from Common Crawl. The vector length is also 300 features. In sum\n",
    "- both trained on very large corpus (100B vs 840B)\n",
    "- both trained on general corpus (Google News vs Common Crawl)\n",
    "- both has 300 features\n",
    "\n",
    "\n",
    "Also note that differences of embeddings in this experiment is not only the models (Word2Vec vs GloVE) but also the data they were trained. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec\n",
      "Best parameters set found on development set:  {'C': 10}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 313.31\n",
      "GloVe\n",
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.84\n",
      "time: 339.56\n"
     ]
    }
   ],
   "source": [
    "# approximate running time: 13 mins\n",
    "\n",
    "print(\"Word2Vec\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"])\n",
    "\n",
    "print(\"GloVe\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"glove\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2 Discussion**\n",
    "\n",
    "\n",
    "```\n",
    "Word2Vec\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "GloVe\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 1000}\n",
    "Best F1 on development set: 0.86\n",
    "F1 on test set: 0.85\n",
    "742.7005350589752\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 - of tfidf\n",
    "\n",
    "In this experiment,...\n",
    "\n",
    "useful for IR but may not for classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n",
      "Best parameters set found on development set:  {'C': 100}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 316.04\n",
      "norm + idf\n",
      "Best parameters set found on development set:  {'C': 100}\n",
      "Best F1 on development set: 0.84\n",
      "F1 on test set: 0.83\n",
      "time: 333.35\n"
     ]
    }
   ],
   "source": [
    "# approximate running time: 12 mins\n",
    "\n",
    "print(\"norm\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"])\n",
    "\n",
    "print(\"norm + idf\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], tfidf=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:  {'C': 10}\n",
      "Best F1 on development set: 0.84\n",
      "F1 on test set: 0.83\n",
      "time: 330.85\n"
     ]
    }
   ],
   "source": [
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "```\n",
    "norm\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 10}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "norm + idf\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 100}\n",
    "Best F1 on development set: 0.83\n",
    "F1 on test set: 0.83\n",
    "677.7894566059113\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 - Pooling\n",
    "\n",
    "While reading the first three experiments in this Notebook, you may be wondering that Word embeddings (Word2Vec and GloVE) are dense representation of \"words\" not \"document\", so how can we come up with vectors representing document? To do so, we have to pool word embeddings, similar idea as pooling layer in CNN. In the first three experiments, we simply average the embeddings of each tokens to get the vector representing the document. While this technique is so simple, it has been widely used. Not only in academic, but industrial NLP library such as spaCy [doc vector](https://spacy.io/api/doc#vector) and [BERT-AS-A-SERVICE](https://github.com/hanxiao/bert-as-service#speech_balloon-faq) also pool a document vector by averaging. \n",
    "\n",
    "\n",
    "However, averaging is not the only way we can pool a document vector. Let's step back a little to the fundamental. What do we do in Notebook 1? We use one-hot encoding to encode a word and then we sum them up! Although our word representation is now embeddings (dense) instead of one-hot encoding (sparse), we can still do the same thing. The reason why averaging is more popular is that it eliminate the effect of document length. For example, these two documents `cat cat dog dog` and `cat dog` will be the same in vector space. Another technique is to use log, as presented in this [book](https://nlp.stanford.edu/IR-book/pdf/06vect.pdf). The idea is to reduce the effect of token that occur many times. For example, the document like `dog dog dog cat` will lean toward `dog` in vector space if we average.  However, it will lean toward `dog` in less degree if we use log pooling technique. However, this log technique is introduced for Information Retrieval context, which is determining the query vector and document vector. As our problem set is text classification, this technique may not work. \n",
    "\n",
    "One may speculate that averaging and summing are pretty much the same since we just multiply vectors with some constants. This may be true for information retrieval since `sim(q, d)` and `sim(q, c x d)` are the same where `sim` is cosine similarity. However, for classification we are to draw a boundary in vector space, and since by taking average we multiply those vectors with different constants (each document can have different length), it can change to decision boundary. \n",
    "\n",
    "Note that all these variations are Bag Of Word, which does not take the position of words into account. In other words `The movie is not good. It is boring` and `The movie is not boring. It is good` are represented with the same vector.\n",
    "\n",
    "In this experiment, we will try three pooling technique: sum, average and log.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n",
      "Best parameters set found on development set:  {'C': 10}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 311.83\n",
      "sum\n",
      "Best parameters set found on development set:  {'C': 0.001}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 2076.04\n",
      "log\n",
      "Best parameters set found on development set:  {'C': 1}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 1185.65\n"
     ]
    }
   ],
   "source": [
    "# approximate running time: 60 mins\n",
    "\n",
    "print(\"norm\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], polling=\"norm\")\n",
    "\n",
    "print(\"sum\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], polling=\"sum\")\n",
    "\n",
    "print(\"log\")\n",
    "_, _ = run_pipeline(dataset, word_emb_models[\"word2vec\"], polling=\"log\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Exp1-4**\n",
    "\n",
    "```\n",
    "\n",
    "norm\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 100}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "sum\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 0.001}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.85\n",
    "log\n",
    "Load tokenized document from disk\n",
    "Load tokenized document from disk\n",
    "Best parameters set found on development set:  {'C': 0.001}\n",
    "Best F1 on development set: 0.85\n",
    "F1 on test set: 0.84\n",
    "3642.0309517383575\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- generalize, not domain specific\n",
    "- too many mssing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_emb_models[\"word2vec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f\" %2.34501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'McCaskell',\n",
       " 'Teresa_Strattard',\n",
       " 'MERCYHURST',\n",
       " 'EDAPS_Consortium',\n",
       " 'fingerpicked_guitar',\n",
       " 'Samhan',\n",
       " 'Hazard_Identification',\n",
       " 'Parithi_Ilamvazhuthi',\n",
       " 'mudslide_prone_mountain',\n",
       " 'Cecafa_Kagame_Cup',\n",
       " 'strum_guitars',\n",
       " 'OSAA_4A',\n",
       " 'GeoCortex_Essentials_insights',\n",
       " 'Columbanus',\n",
       " 'Bibiani_Ghana',\n",
       " 'cataloger_retailer',\n",
       " 'Hadith_sayings',\n",
       " 'COACH_MILES',\n",
       " 'Omar_Leary',\n",
       " 'Zimdars',\n",
       " 'Motosport',\n",
       " 'disentangling_itself',\n",
       " '0w_####m',\n",
       " 'LOSS_BEFORE_INCOME_TAXES',\n",
       " 'Dante_Walkup',\n",
       " 'Barkha',\n",
       " 'girl_mother_Nixzaliz',\n",
       " 'Nissha',\n",
       " 'Walior',\n",
       " 'non_tariff_barriers',\n",
       " 'FEATHER_RIVER',\n",
       " 'Nomlaki',\n",
       " 'Charging_Cradle',\n",
       " 'bio_diesels',\n",
       " 'CRATE',\n",
       " 'Minister_Atzo_Nicolai',\n",
       " 'Mazze',\n",
       " 'Alumni_Awards_Banquet',\n",
       " 'inseparably_linked',\n",
       " 'Indridson',\n",
       " 'Qorivva',\n",
       " 'Rasho_Nesterovic_Maceo_Baston',\n",
       " 'Referee_Alex_Prus',\n",
       " 'Volrath',\n",
       " 'Jeanette_Liebold_Ricker',\n",
       " 'Gien',\n",
       " '£_7bn',\n",
       " 'ayurvedic_medicines',\n",
       " 'Wheelchair_racer',\n",
       " 'plush_leather_couches',\n",
       " 'By_AAyles',\n",
       " 'SPLENDA_®_Sucralose',\n",
       " 'critic',\n",
       " 'Fathi_Terbil',\n",
       " 'Bobby_Doerr',\n",
       " '###-TESTIGO',\n",
       " 'Mikka_Hakkinen',\n",
       " 'Binikos',\n",
       " 'Wangita',\n",
       " 'Sarah_Bloom_Raskin',\n",
       " 'corel_draw_x4',\n",
       " 'WEWAHITCHKA_Fla.',\n",
       " 'Royal_Armouries',\n",
       " 'congenitally',\n",
       " 'awkward_bedfellows',\n",
       " 'fumbler',\n",
       " 'hemodynamic',\n",
       " 'conceives_twins',\n",
       " 'Donnino',\n",
       " 'Malay_Mail_tag',\n",
       " 'iSONEP',\n",
       " 'Omar_Kodmani_senior',\n",
       " 'ají',\n",
       " 'BZ2',\n",
       " 'felon_baboons',\n",
       " 'Martijn_Wilder',\n",
       " 'Anurag_Wadehra',\n",
       " 'Sanctuaries',\n",
       " 'Brozova',\n",
       " 'carbon_nanotube_interconnects',\n",
       " 'TREVOSE_Pa._BUSINESS_WIRE',\n",
       " 'Santo_António',\n",
       " 'silk_organza_skirt',\n",
       " 'PC_WorldBench',\n",
       " 'Ivo_Josipovic_garnered',\n",
       " 'Seick',\n",
       " 'infamous_Arbeit_Macht_Frei',\n",
       " 'Hilton_Head_Humane',\n",
       " 'Bebe_Neuwirth',\n",
       " 'Oloffson',\n",
       " 'MONTREAL_Astral_Media',\n",
       " 'quadrature_output',\n",
       " 'Bridal_Fashions',\n",
       " 'merrily_whistled_sang',\n",
       " 'Mian_Ajmal',\n",
       " 'Boots_BOOT.L_Quote_Profile',\n",
       " 'Alexian_Brothers_Medical',\n",
       " 'Parkson_Retail_Group',\n",
       " 'include_uncoated_papers',\n",
       " 'Team_DMAT',\n",
       " 'Padres_catcher_Yorvit',\n",
       " 'ASX_WCU',\n",
       " 'birds_chirp',\n",
       " 'Mark_Jakins',\n",
       " 'Old_Firm_rivals',\n",
       " 'BRF',\n",
       " '#.##/oz_Ag',\n",
       " 'Baadasssss',\n",
       " 'Moonacre',\n",
       " 'sc',\n",
       " 'Laksana_Tiranarat',\n",
       " 'kitesurf',\n",
       " 'Matt_Moley',\n",
       " 'Srecko_Juricic',\n",
       " 'waterway_narrow_chokepoint',\n",
       " 'isodes',\n",
       " 'Ghost_Whisper',\n",
       " 'Tiekoumala',\n",
       " 'Scherer',\n",
       " '##--LOWELL',\n",
       " 'BRADY_MYEROV',\n",
       " 'Victor_Scarano',\n",
       " 'Ilaga',\n",
       " 'rocky_ridge',\n",
       " 'SPEARs',\n",
       " 'Ganet',\n",
       " 'Tapscott',\n",
       " 'Coffeemaker',\n",
       " 'PRNewswire_brokersXpress_LLC',\n",
       " 'Amir_Cheema',\n",
       " 'MCET',\n",
       " 'garlanding',\n",
       " 'Gov._Joseph_Marañon',\n",
       " 'self_proclaimed_mastermind_Khalid',\n",
       " 'TEVI',\n",
       " 'fumé_blanc',\n",
       " 'Dale_Campbell_Savours',\n",
       " 'Capt._Muthana',\n",
       " 'Mr_Tampoe',\n",
       " 'V###w',\n",
       " 'Memory_Loss',\n",
       " 'beatnik_poet',\n",
       " 'reorders',\n",
       " 'sinus_syndrome',\n",
       " 'Kerviel',\n",
       " 'JAG',\n",
       " 'Flan',\n",
       " 'Linda_Hirneise_executive',\n",
       " 'Banovetz',\n",
       " 'Sanatana',\n",
       " 'Muntaner',\n",
       " 'Mac_Pherson',\n",
       " 'avid_birders',\n",
       " 'Nina_Stemme',\n",
       " 'Gdynia',\n",
       " 'AOCN',\n",
       " 'Boozoo',\n",
       " 'Yaz_Yasmin_Seasonale_Seasonique',\n",
       " 'cryptogenic_stroke',\n",
       " 'Mangga_Dua',\n",
       " 'Underage_drinking',\n",
       " 'Maxoderm',\n",
       " 'Slywotzky',\n",
       " 'VGI',\n",
       " 'Devon_Herrick',\n",
       " 'Wakan',\n",
       " 'Fidelis_Tapgun',\n",
       " 'TimeSight_VLM',\n",
       " 'DOUBTFUL_CB',\n",
       " 'Bracks_Brumby',\n",
       " 'przez',\n",
       " 'S###A',\n",
       " 'please_visit_www.pacgenbiopharm.com',\n",
       " 'Jake_Vander_Zanden',\n",
       " 'BFi',\n",
       " 'Thou_art',\n",
       " 'Tod_Bunting',\n",
       " 'By_Jeff_Patrus',\n",
       " 'John_Lichman_despite',\n",
       " 'NUPTE',\n",
       " 'Namba',\n",
       " 'Accounts_receivable_net',\n",
       " 'TLC_Laser',\n",
       " 'ALIAS',\n",
       " 'Madison.com_sports',\n",
       " 'Johnnie_Swartz',\n",
       " 'JIMMIE_JOHNSON_Yeah',\n",
       " 'video_cassette_recorder',\n",
       " 'LA_PAZ_Bolivian',\n",
       " 'Jaycees',\n",
       " 'Defence_Zahid_Hamid',\n",
       " 'difficulty_swallowing',\n",
       " 'Jikei_University',\n",
       " 'Shennongjia',\n",
       " 'Dente',\n",
       " 'Merrimack',\n",
       " 'Animal_Diagnostic_Laboratory',\n",
       " 'Raju_Ananthaswamy',\n",
       " 'Lovers_Key',\n",
       " 'thrifting',\n",
       " 'grievously_harmed',\n",
       " 'wishful_thinking',\n",
       " 'forecaster_Arun_Raha',\n",
       " 'Jonathan_Bricklin',\n",
       " 'Pesticide_Applicator',\n",
       " 'postcards',\n",
       " 'TUESDAY_DEC',\n",
       " 'ColonSentry_test',\n",
       " 'varying',\n",
       " 'Minister_Elias_Murr',\n",
       " 'DLCD',\n",
       " 'Deatrich',\n",
       " 'Outdoor_Grills',\n",
       " 'Touchstone_Advisors',\n",
       " 'Morenstein',\n",
       " 'Lissiman',\n",
       " 'Jonathan_Swindlehurst',\n",
       " 'radio-frequency/wireless_devices',\n",
       " 'Minefinders_Annual',\n",
       " 'analyst_Dirk_Peeters',\n",
       " 'Heart_###.#FM',\n",
       " \"Daniel_D'_Aniello\",\n",
       " 'staunch_royalist',\n",
       " 'mce',\n",
       " 'niggle',\n",
       " 'Fodors.com',\n",
       " 'Salvador_Benedicto',\n",
       " 'suspensions',\n",
       " 'WPP_Kantar_Media',\n",
       " 'km_2D_seismic',\n",
       " 'Dongbu_HiTek_Co.',\n",
       " 'spokesman_Ning_Shuyong',\n",
       " 'Society_MSMS',\n",
       " 'ARE_AVAILABLE_ON',\n",
       " 'Tai_Tokerau_electorate',\n",
       " 'Rachel_Pomerance',\n",
       " 'Akashic_Records',\n",
       " 'Temelin',\n",
       " 'Hurting',\n",
       " 'Cadw',\n",
       " 'iMoneyNet_publisher',\n",
       " 'section_##CCC',\n",
       " 'cotton_twill_denim',\n",
       " 'SCHAUMBURG_FLYERS',\n",
       " '.##_caliber_semiautomatic',\n",
       " 'Pheasants_Forever_chapter',\n",
       " 'Credit_Agricole_CAGR.PA',\n",
       " 'Owusu_Ansah',\n",
       " 'CONAN_Yeah',\n",
       " 'OptumHealth_UnitedHealth_Group',\n",
       " 'HIV_Aids_Tuberculosis',\n",
       " 'Oloya',\n",
       " 'Segmental_Analysis',\n",
       " 'Mattea',\n",
       " 'Thomas_von_Heesen',\n",
       " 'Councilwoman_Karin_Uhlich',\n",
       " 'Kurt_Damrow',\n",
       " 'BY_RICHARD_RYMAN',\n",
       " 'EXECUTIVE_SECRETARY',\n",
       " 'PROHIBITED',\n",
       " 'filmmaker_Paul_Greengrass',\n",
       " 'especies',\n",
       " 'JAMAAT_e_Islami',\n",
       " 'Cutbank_Complex',\n",
       " 'boro_cultivation',\n",
       " 'Farzad_Nazem_Yahoo',\n",
       " 'Dame_Gwyneth',\n",
       " 'THANG',\n",
       " 'San_Marino_Corned',\n",
       " 'Glou',\n",
       " 'AMONG_THEM',\n",
       " 'slumbering_bats',\n",
       " 'Everybody',\n",
       " 'Ditmer',\n",
       " 'bookman',\n",
       " 'farmouts',\n",
       " 'Zumba_fitness',\n",
       " 'Yuliya_Tymoshenko_Yushchenko',\n",
       " 'burlap_bag',\n",
       " 'senescent',\n",
       " \"La_femme_l'\",\n",
       " 'Van_Osdol',\n",
       " 'Kostyantin_Zhevago',\n",
       " 'Emulsion_Polymers',\n",
       " 'Barbiturates',\n",
       " 'News##_meteorologist',\n",
       " 'Etihad_Guest',\n",
       " 'Wrox',\n",
       " 'Ingenieria',\n",
       " 'Genre',\n",
       " 'backed_Enterprise_Turnaround',\n",
       " 'Marrow_Transplantation',\n",
       " 'Dhillon',\n",
       " 'Padmaja',\n",
       " 'St_Aelred',\n",
       " 'RiverTown_Crossings',\n",
       " 'LIBOR_LIBOR',\n",
       " 'pinup_poster',\n",
       " 'Ben_Yahtzee_Croshaw',\n",
       " 'BrightSide',\n",
       " 'Grupo_ASSA',\n",
       " 'Mimi_Tran',\n",
       " 'berms_earthen_barriers',\n",
       " 'Mustang_Camaro',\n",
       " 'SHERRY_JACOBSON',\n",
       " 'Corporation_TSX_JEX',\n",
       " 'lower_houseThe_Rudd',\n",
       " 'Larne_Lough',\n",
       " 'Westinghouse_Electric',\n",
       " 'exim',\n",
       " 'Found_ation',\n",
       " 'WEDA',\n",
       " 'GOOD_VIBRATIONS',\n",
       " 'Brent_Shepheard_Shawnigan_Lake',\n",
       " 'Samuel_Waksal_pleaded_guilty',\n",
       " 'Alajajian',\n",
       " 'Handel_Bach',\n",
       " 'response_Abe_Radkin',\n",
       " 'aristocrats_sipped_champagne',\n",
       " 'Hasan_Abdel_Fattah',\n",
       " 'student_Don_Coorough',\n",
       " 'outfielder_Tuffy_Rhodes',\n",
       " 'QT_Financing',\n",
       " 'Adolescence',\n",
       " 'scrumptious_goodies',\n",
       " 'Carlos_Delgado_Moises_Alou',\n",
       " 'Stocktrail',\n",
       " 'Ferodo',\n",
       " 'ClickPay_Solutions_Inc.',\n",
       " 'Ronny_Rosenthal',\n",
       " 'Whiteman_Tennis',\n",
       " 'TBTF',\n",
       " 'Wien_Austria',\n",
       " 'spokeswoman_Meg_Olberding',\n",
       " 'Toni_Senecal',\n",
       " 'Ferdinando_Imposimato',\n",
       " 'Liam_Roybal',\n",
       " 'IAN_MOHR',\n",
       " 'Solamar',\n",
       " 'CROCS',\n",
       " 'Denmark_Rasmus_Henning',\n",
       " 'homeloans',\n",
       " 'CC_##mins_##secs',\n",
       " 'IRCA',\n",
       " 'Santa_Claus',\n",
       " 'W._Guidara',\n",
       " 'Berels',\n",
       " 'controlled_doorbell_ringer',\n",
       " 'Xuzhou_Jiangsu_province',\n",
       " 'Vincent_Belnome',\n",
       " 'Koldo',\n",
       " 'Laugerud',\n",
       " 'TUMBLES',\n",
       " 'SearchMedia_beliefs',\n",
       " 'Bravada',\n",
       " 'Pratt_Bannatyne',\n",
       " 'Tugas',\n",
       " 'Tod_Cooperman_MD',\n",
       " 'sailfish',\n",
       " 'idiopathic_generalized_epilepsy',\n",
       " 'Lompoc',\n",
       " '---------_---------_Net',\n",
       " 'False_alarms',\n",
       " 'Edition_MCE',\n",
       " 'Ron_Swarner',\n",
       " 'kosher_butchers',\n",
       " 'Schmaljohn',\n",
       " 'Van_Buskirk_Golf_Course',\n",
       " 'WILLOW_Alaska_Alaska',\n",
       " 'BMJ',\n",
       " 'STROUPE',\n",
       " 'svpeople@mercurynews.com',\n",
       " 'GNS_###W',\n",
       " 'Tao_Dong',\n",
       " 'kissed_demurely',\n",
       " 'Spencer_Nairn',\n",
       " 'stock.The',\n",
       " 'Boed',\n",
       " 'Moses_Katumbi',\n",
       " 'Rhomobile_Rhodes',\n",
       " 'Sachin_Tendulkar_Rahul_Dravid',\n",
       " 'Cafe_Internationale',\n",
       " 'Youssouf_Saleh',\n",
       " 'Tom_Brolley_Staff_Writer',\n",
       " 'Diane_Hofstede',\n",
       " 'Binh_Phuoc',\n",
       " 'Antegren',\n",
       " 'Bill_Oketch',\n",
       " 'Actress_Telma_Hopkins',\n",
       " 'Marmosets',\n",
       " 'AND_OTHER_REASONS_INVESTORS',\n",
       " 'Inc_CLMS',\n",
       " 'OTCQX_AVARF',\n",
       " 'Lt._Gen._Raymundo_Ferrer',\n",
       " 'Muktsar',\n",
       " 'Vitatron',\n",
       " 'Hitler_Wehrmacht',\n",
       " 'Nazeer_Cassim',\n",
       " 'dictator_Slobodan_Milosevic',\n",
       " 'Robert_Jablon_Daisy_Nguyen',\n",
       " 'Comp_Rehab',\n",
       " 'Colin_Ellar',\n",
       " 'Urinary_Incontinence',\n",
       " 'Sandro_Petraglia',\n",
       " 'Kulayigye',\n",
       " 'potato_salad_pasta_salad',\n",
       " 'ZONA_Design',\n",
       " 'Lib_Cons',\n",
       " 'Extraction',\n",
       " 'Eve_Alvord',\n",
       " 'valleys',\n",
       " 'shower_fixtures_KraftMaid',\n",
       " 'www.cleco.com_select_For',\n",
       " 'Şanlıurfa',\n",
       " 'Jacob_Skeans',\n",
       " 'WAIMEA',\n",
       " 'flop_Gerica',\n",
       " 'induced_polarization_resistivity',\n",
       " 'cicumstances',\n",
       " 'Wacom_patented',\n",
       " 'Entraction',\n",
       " 'Surraya',\n",
       " 'Tlatelolco_square',\n",
       " 'carmaker_Gaz',\n",
       " 'TEHRAN_Expediency_Council',\n",
       " 'FINANCIAL_REPORTING',\n",
       " 'Autobody_Repair',\n",
       " 'Kelsey_Ansley',\n",
       " 'kiev',\n",
       " 'Jake_Bolitho',\n",
       " 'Katie_Hogwood',\n",
       " 'prevention_DLP',\n",
       " 'adviser_Eladio_Parames',\n",
       " 'DS_Senanayake_College',\n",
       " 'Willows_Unified',\n",
       " 'minus_2C',\n",
       " 'Bishop_Carlito_Cenzon',\n",
       " 'analyzes_exhaled_anesthetic',\n",
       " 'Anil_Mehta',\n",
       " 'Salmona',\n",
       " 'neoadjuvant_chemoradiotherapy',\n",
       " 'Dera_Ghazi',\n",
       " 'champion_Petr_Korda',\n",
       " 'HM_Treasury',\n",
       " 'John_A._Elolf',\n",
       " 'Unit_FRU',\n",
       " 'Meijer',\n",
       " 'Oodachee',\n",
       " 'Minneapolis_MN_PRWeb',\n",
       " 'Privateers',\n",
       " 'Baghlia',\n",
       " 'Befekadu',\n",
       " 'Jerome_Eckrich',\n",
       " 'SCRP',\n",
       " 'petty_irritations',\n",
       " 'William_Randolph_Hearst',\n",
       " 'Crikey',\n",
       " 'Rookie_receiver_Darrius',\n",
       " 'William_Kruziki',\n",
       " 'Accumulated',\n",
       " 'Si_Si',\n",
       " 'restimulate',\n",
       " 'Moorefield',\n",
       " 'Souleye',\n",
       " 'dissuasive',\n",
       " 'AfNat_Resources_Limited',\n",
       " 'Possible_Salmonella_Contamination',\n",
       " 'vitriolic_rhetoric',\n",
       " 'Depaz',\n",
       " 'Peotona',\n",
       " 'Belly_Up',\n",
       " 'Clonegal',\n",
       " 'Kata_Noi',\n",
       " 'lok_sabha',\n",
       " 'DVOE',\n",
       " 'AMERICAN_COMMUNITY_BANCORP_INC.',\n",
       " 'insulin_syringes',\n",
       " 'medevac_chopper',\n",
       " 'rookie_Laurenne_Ross',\n",
       " 'Rockies_starter_Esmil',\n",
       " 'barnyard',\n",
       " 'Decorize',\n",
       " 'WAXHAW',\n",
       " 'Mobiso',\n",
       " 'Shareholder_Suit',\n",
       " 'Fahmi_Mochtar',\n",
       " 'organic_manures',\n",
       " 'China_Shengda_Packaging',\n",
       " 'administrator_Marion_Blakey',\n",
       " 'Frank_Falzarano',\n",
       " 'pungent_aromas',\n",
       " 'Brawley',\n",
       " 'Compsee',\n",
       " 'Mahi_Ve',\n",
       " 'Durka',\n",
       " 'Tanbark',\n",
       " 'Task_Force_IETF',\n",
       " 'Valda',\n",
       " 'GamePlan',\n",
       " 'moist_snuff',\n",
       " 'Dossantos',\n",
       " 'fake_identifications',\n",
       " 'transarterial',\n",
       " 'Syracuse_DB_Tanard',\n",
       " 'Randy_Fontanez',\n",
       " 'Minister_Li_Xueju',\n",
       " 'NBCN',\n",
       " 'humiliatingly',\n",
       " 'SNMTS',\n",
       " 'Megan_Hilson',\n",
       " 'Brigadier_r',\n",
       " 'cyclic_carbonates',\n",
       " 'China_ICBC_####.HK',\n",
       " 'Dynasign',\n",
       " 'Martin_Puris',\n",
       " 'Richard_von_Krafft',\n",
       " \"sa_isa't_isa\",\n",
       " 'Tony_Scotta_Macomb',\n",
       " 'NASDAQ_IXYS',\n",
       " 'WWE_Cruiserweight_Champion',\n",
       " 'materially_adversely_affect',\n",
       " 'Paseo_Del',\n",
       " 'Paula_Steib_spokeswoman',\n",
       " 'Cornerback_Daylon_McCutcheon',\n",
       " 'Hilton_Lisle_Naperville',\n",
       " 'TriMark',\n",
       " 'Loss_Statement_H.4',\n",
       " 'Pedrett',\n",
       " '_She',\n",
       " 'Jan_van_Goyen',\n",
       " 'Renaud_Beauchesne',\n",
       " 'Alpha_Kappa_Rho',\n",
       " 'Wyndmere',\n",
       " 'MySQL_PostgreSQL',\n",
       " 'Nansen_Saleri',\n",
       " 'CHAPARRAL_NM',\n",
       " 'Pierre_Zalloua',\n",
       " 'Aksy',\n",
       " 'Bikash_Bhattacharya',\n",
       " '---------------------------------_ASSETS',\n",
       " 'Skyla_Whitaker',\n",
       " 'Owen_Braugher',\n",
       " 'Chairman_Max_Baucus',\n",
       " 'Don_Polye',\n",
       " 'Wat_Pho',\n",
       " 'Kentrell_Gransberry',\n",
       " 'Silvio_Danailov',\n",
       " 'Bohannan_Ry',\n",
       " 'Oshin',\n",
       " 'École_Nationale_Supérieure_des',\n",
       " 'Anthony_DiComo',\n",
       " 'Policy_Orchestration',\n",
       " 'Jetta_GLI',\n",
       " 'posteriori',\n",
       " 'analyst_Stanislav_Belkovsky',\n",
       " 'ICOP_newest',\n",
       " 'loran',\n",
       " \"debut_album_Quelqu'un\",\n",
       " 'spurned_suitor_Microsoft',\n",
       " 'underworld_don',\n",
       " 'Thear',\n",
       " 'Lil_Romeo',\n",
       " 'Llanddewi_Brefi',\n",
       " 'WARRINGTON_Wolves',\n",
       " 'Insurance_SPC_CISPC',\n",
       " 'law_Melanie_Burwell',\n",
       " 'EH_Crump',\n",
       " 'Chee_Hoe',\n",
       " 'Strelchenko_sued',\n",
       " 'Populus_interviewed_random',\n",
       " 'Greek_polis',\n",
       " 'Briyana_Blair',\n",
       " 'year.##',\n",
       " 'Pokemon_Diamond',\n",
       " 'Khoshbin',\n",
       " 'VERINT',\n",
       " 'babbled',\n",
       " 'Slappy',\n",
       " 'Surgical_Matrix',\n",
       " 'REUTERS_Brendan_Mcdermid',\n",
       " 'WQYK_FM',\n",
       " 'rRNA_genes',\n",
       " 'VIRUS_Warlock_seven',\n",
       " 'Desiray',\n",
       " 'Ronel',\n",
       " 'Martin_Ignasiak',\n",
       " 'Rockaway_Turnpike',\n",
       " 'AAM_Co_Founder',\n",
       " 'CONMED_Corporation_NASDAQ_CNMD',\n",
       " 'Sunsail_clubs_holidays',\n",
       " 'DENVER_Qwest_Communications',\n",
       " 'Rudi_Fronk',\n",
       " 'NHRA_drag',\n",
       " 'Dunbar',\n",
       " 'Omor',\n",
       " 'Fliger',\n",
       " 'ISSI_Intelli_Site',\n",
       " 'AAMIR',\n",
       " 'Prof._Oserheimen_Osunbor',\n",
       " 'POWAY_Calif._BUSINESS_WIRE',\n",
       " 'Greg_Nasello',\n",
       " 'Craig_Breslow',\n",
       " 'Demon_Claws',\n",
       " 'ROSELAND_NJ_Marketwire_##/##/##',\n",
       " 'Inc._OTCBB_ITRO',\n",
       " 'Hassan_Fofana',\n",
       " 'al_Farhoud',\n",
       " 'Michael_Zacharatos',\n",
       " 'Wamboldt',\n",
       " 'Dr._Rickard_Ljung',\n",
       " 'diseas',\n",
       " 'desert_camouflage_fatigues',\n",
       " 'Kevin_Cloe_Publisher',\n",
       " 'convic_tion',\n",
       " 'Smithfield_Foods_Helping',\n",
       " 'Hodges_Dunlap',\n",
       " 'www.zazzle.com',\n",
       " 'maple_walnut_ice_cream',\n",
       " 'nephew_Doug_Kalitta',\n",
       " 'Rebekah_Lampman_Fort_Hood',\n",
       " 'GBTA',\n",
       " 'Headingly',\n",
       " 'Gene_Wier',\n",
       " 'THB#.#_billion',\n",
       " 'Thoma_Funeral_Home',\n",
       " 'heroes',\n",
       " 'Sudharani',\n",
       " 'Friend_Neil_Aspinall',\n",
       " 'Barne_Barton',\n",
       " 'Baranek',\n",
       " 'Prosilica',\n",
       " 'Trust_KST_Declares',\n",
       " 'Analogic',\n",
       " 'Owers',\n",
       " 'Devaki',\n",
       " 'David_Stempler',\n",
       " 'Heather_Ranck',\n",
       " 'Stiggy_Honda',\n",
       " 'AEP_Appalachian',\n",
       " 'digusting',\n",
       " 'quartz_monzonite_porphyry',\n",
       " 'Cefail',\n",
       " 'Geoffrey_Hewings_director',\n",
       " 'Hapney',\n",
       " 'trans_fats_saturated_fats',\n",
       " 'Jeannine_Stein',\n",
       " 'Trisko',\n",
       " 'Ghajn',\n",
       " 'Windthorst',\n",
       " 'Carol_Oates',\n",
       " 'IN_THE_APPAREL',\n",
       " 'EcaFlo',\n",
       " 'Marissa_Kazbour',\n",
       " 'NAS',\n",
       " 'unintelligibility',\n",
       " 'James_Kerasiotes_resigned',\n",
       " 'Onyonka',\n",
       " 'Nora_Volkow',\n",
       " 'pitcher_Jonny_Venters',\n",
       " 'Wang_Zhenghua',\n",
       " 'Moor_Nook',\n",
       " 'Air_Freight_Forwarder',\n",
       " 'BRAINTREE',\n",
       " 'Jon_Sitko',\n",
       " 'Ratu_Timoci',\n",
       " 'USA_Symbol_FRTE',\n",
       " 'Francesco_Damiani',\n",
       " 'Khalefa',\n",
       " 'Abdol_Hossein',\n",
       " 'Andrew_Compart',\n",
       " 'Kunduz_province_Archi',\n",
       " 'Nuptse',\n",
       " 'Streisand',\n",
       " 'ONTARIO_CCNMatthews',\n",
       " 'Freedline',\n",
       " 'labradors',\n",
       " 'Machine_Rhino',\n",
       " 'Enfo',\n",
       " 'AFPA',\n",
       " 'Punishing',\n",
       " 'Robert_Brustein',\n",
       " 'Colegate',\n",
       " 'Classical_Archaeology',\n",
       " 'Kovtun',\n",
       " 'Socio_Economic_Assessment',\n",
       " '¢_##.###_colones',\n",
       " 'Ken_Schrader_Dodge',\n",
       " 'RK_Verma',\n",
       " 'BBY_Best_Buy',\n",
       " 'Halliburton_HAL_NYSE',\n",
       " 'Guar_Gum',\n",
       " 'Shirli_Sitbon',\n",
       " 'sealing_leaky_ducts',\n",
       " 'Hazardous_Substance',\n",
       " 'BERTH',\n",
       " 'NEW_YORK_EON_Enhanced',\n",
       " 'Musici',\n",
       " 'consortium_Sematech',\n",
       " 'WERE_ALL',\n",
       " 'transthyretin_mediated_amyloidosis_ATTR',\n",
       " 'Donor_Recruitment_Representative',\n",
       " 'Barcia',\n",
       " 'Kalanianaole',\n",
       " 'lgbt',\n",
       " 'Subordinated_Convertible',\n",
       " 'Oct.##_GNA',\n",
       " 'Teshekpuk_Lake',\n",
       " \"Ala'a\",\n",
       " 'Nytro',\n",
       " 'Ante_Sapina_brothers',\n",
       " 'o_holics',\n",
       " 'No._#####-###',\n",
       " 'Once_Bitten_Twice',\n",
       " 'Jayna_Hutchinson',\n",
       " 'Epe',\n",
       " 'Andy_Smeathers',\n",
       " 'BY_TOM_ENLUND',\n",
       " 'Karbasi',\n",
       " 'South_Cerney_Gloucestershire',\n",
       " 'ITN_Layer',\n",
       " 'Fatooh',\n",
       " 'General_Tso_chicken',\n",
       " 'KPLC',\n",
       " 'Corp._ETR.N_Quote',\n",
       " 'amphioxus',\n",
       " 'Apranga',\n",
       " 'Kimberly_Little_Chute',\n",
       " 'Maxle',\n",
       " 'HNO',\n",
       " 'Labour_Eamon_Gilmore',\n",
       " 'spastic',\n",
       " 'Daedalus_Capital',\n",
       " 'Biloxi_Marshes',\n",
       " 'GE_#.#xl_wind_turbines',\n",
       " 'ANDRE_BAGOO',\n",
       " 'Ofir',\n",
       " 'Introduces_Groundbreaking',\n",
       " 'gujarat',\n",
       " 'Eshan_Ul_Haq',\n",
       " 'donut_chain',\n",
       " 'By_Gavin_Gibbon',\n",
       " 'MasterChef_contestant',\n",
       " 'Board_MDXG',\n",
       " 'sermon_Pfleger',\n",
       " 'Nancy_Cartwright',\n",
       " 'Marinel_R._Cruz',\n",
       " 'Fiesta_Parade_Floats',\n",
       " 'BY_MICK_MCCABE',\n",
       " 'wellestablished',\n",
       " 'Broadband_Enabled',\n",
       " 'al_Nassiri',\n",
       " 'Daljeet_Singh',\n",
       " 'venison_carpaccio',\n",
       " 'Runako_Morton_Denesh_Ramdin',\n",
       " 'Okolo_Talib',\n",
       " 'Travis_Hafner',\n",
       " 'economist_Jorgen_Elmeskov',\n",
       " 'Galligos',\n",
       " 'Artificial_Liver',\n",
       " 'Impax_Pharmaceuticals_division',\n",
       " 'Tropical_Meteorology_IITM',\n",
       " 'Apam_Catholic',\n",
       " 'Sri_Swamy',\n",
       " 'Visit_EchoStar_DISH',\n",
       " 'remodeled_Wisp_Resort',\n",
       " 'spokeswoman_Colendula_Green',\n",
       " 'MultiAir',\n",
       " 'zuppa',\n",
       " 'Batchu',\n",
       " 'Subscription_Required',\n",
       " 'waterproof_jumpsuits',\n",
       " 'stablity',\n",
       " 'Nucryst_Pharmaceuticals',\n",
       " 'liras',\n",
       " 'Folegandros',\n",
       " 'Peter_Soeth',\n",
       " 'Soumakian',\n",
       " 'Chandam',\n",
       " 'Skeena_Bulkley',\n",
       " 'Thriving_Ivory',\n",
       " 'Professor_Sheila_Tlou',\n",
       " 'spokesman_Giorgi_Tchanishvili',\n",
       " 'Connesson',\n",
       " 'ion_collider',\n",
       " 'Rampravesh_Rai',\n",
       " 'Shimabukuro',\n",
       " 'rich_Niger_delta',\n",
       " 'Bruker',\n",
       " 'Finke',\n",
       " 'Vanhooser',\n",
       " 'HPLF',\n",
       " '--------------------------------_Subtotal',\n",
       " 'Mainardi',\n",
       " 'Becali',\n",
       " 'Strap_Match',\n",
       " 'Albufera',\n",
       " 'mass_mobilisations',\n",
       " 'homemade_sangria',\n",
       " 'UPI_Wire',\n",
       " 'Mapfre_Aspar_rider',\n",
       " 'MECHANIC_Mechanic_Needed',\n",
       " 'WW_Grainger_GWW',\n",
       " 'Carless',\n",
       " 'MICHEL_SPINGLER',\n",
       " 'Curt_Cavin_covers',\n",
       " 'wool_sweater',\n",
       " 'boss_Bernie_Ecclestone',\n",
       " 'servo_actuator',\n",
       " 'Sue_Ter_Maat',\n",
       " 'Jackie_Condolences',\n",
       " 'Sholanda',\n",
       " 'Mohsin_Nathani',\n",
       " 'Newtson',\n",
       " 'Ryan_Sweeting',\n",
       " 'Abba_Sayyadi_Ruma',\n",
       " 'pixel_capacitive_touchscreen',\n",
       " 'pacemen_Glenn_McGrath',\n",
       " 'Henry_Hungerbeeler',\n",
       " 'Swine_Flu_Precautions',\n",
       " 'efficent',\n",
       " 'Marnell_Corrao',\n",
       " 'Francoise_Shenfield_infertility',\n",
       " 'Religions',\n",
       " 'McData_switches',\n",
       " 'Kattegat',\n",
       " 'Your_Rupture',\n",
       " 'By_Brent_Mutis',\n",
       " 'Carl_Elmore',\n",
       " 'beaded_choker',\n",
       " 'Dr._Pius_Okigbo',\n",
       " 'Stets',\n",
       " 'Aliviado',\n",
       " 'Commissioner_Geoghegan_Quinn',\n",
       " 'violent_Basque_separatism',\n",
       " 'HSGP',\n",
       " 'HEATING',\n",
       " 'Trevor_Siemian',\n",
       " 'Max_Manus',\n",
       " 'Analects',\n",
       " 'StatoilHydro_STL.OL_Quote_Profile',\n",
       " 'Filippo_Lippi',\n",
       " 'Ipecac',\n",
       " 'undeclared_superdelegates_express',\n",
       " 'Butte_LaRose_Krotz_Springs',\n",
       " 'Asus_G##V',\n",
       " 'nonsupervisory_workers',\n",
       " 'Teutons',\n",
       " 'Los_Llanos',\n",
       " 'Woodview_Apartments',\n",
       " 'Bowdy',\n",
       " 'Karkare',\n",
       " 'ignition_interlock_devices',\n",
       " 'Hayleys_MGT_Knitting',\n",
       " 'Rokos',\n",
       " 'Mahasarakham_University',\n",
       " 'Capt._Ciro_Chimento',\n",
       " 'Ognyan_Gerdzhikov',\n",
       " 'Ndaba',\n",
       " 'Terrorism_Unconventional_Threats',\n",
       " '3m_springboard_diving',\n",
       " 'Pateke_3H',\n",
       " 'PESTLE_segments',\n",
       " 'prolific_Porcupine_Destor',\n",
       " 'Guidoboni',\n",
       " 'de_Cervantes',\n",
       " 'Duguay',\n",
       " 'Bellish',\n",
       " 'MBUSI',\n",
       " 'Inmate_Execution',\n",
       " 'Anas_Mansour',\n",
       " 'Cabot_Oil',\n",
       " 'Vladimir_Zhirinovsky_Liberal',\n",
       " 'Dixen',\n",
       " 'Metcash_Trading_Ltd.',\n",
       " 'Kaeb',\n",
       " 'COHR',\n",
       " 'Diet_Tricks',\n",
       " 'Rapper_Tupac_Shakur',\n",
       " 'Diana_Damrau',\n",
       " 'Pollokshields',\n",
       " 'Iad',\n",
       " 'Diagnostics_Tektronix',\n",
       " 'Ted_Ammon',\n",
       " 'GENE_TRAINOR',\n",
       " 'ARSS_Infrastructure',\n",
       " 'http://www.vishay.com',\n",
       " '™_ll',\n",
       " 'Inviational',\n",
       " 'Gaustad_Buf',\n",
       " 'Waldo_Mariscal_Pelaez',\n",
       " 'optout',\n",
       " 'fallen_comrade',\n",
       " 'Sciatica',\n",
       " 'THIS_MATERIAL',\n",
       " 'Chris_Unterstein',\n",
       " 'Trade_German_Gref',\n",
       " 'Coda_Octopus_Group',\n",
       " 'manager_Ronnie_McFall',\n",
       " 'Devaru_Kotta_Thangi',\n",
       " 'composer_Leonard_Bernstein',\n",
       " 'Amirsys',\n",
       " 'Dayton_Peace_Accords',\n",
       " 'Oleg_Ostapenko',\n",
       " 'ETDRS_eye_chart',\n",
       " 'Commissioner_Laura_Fortman',\n",
       " 'Wagon_Days',\n",
       " 'Prolly',\n",
       " 'utter_peep',\n",
       " 'divalike_behavior',\n",
       " 'Kay_Kneen',\n",
       " 'Melchert_Dinkel_posed',\n",
       " '@_fresno_bee.com',\n",
       " 'poolside_cabana',\n",
       " 'anthropoids_evolved',\n",
       " 'Acpo',\n",
       " 'flatscreen_televisions',\n",
       " 'Nemorin',\n",
       " 'revolutionary_LZR_Racer',\n",
       " 'actionability',\n",
       " 'IR1',\n",
       " 'Totalsports_Challenge',\n",
       " 'Plemmons',\n",
       " 'Mulry',\n",
       " 'Chapt',\n",
       " 'Ding_Ning',\n",
       " 'Tareq_Khalil',\n",
       " 'Little_Chalfont',\n",
       " 'Damani_Corbin',\n",
       " 'EASTWEST',\n",
       " 'Judge_Alex_Kozinski',\n",
       " 'Lyndaker',\n",
       " 'Kala_Yoders',\n",
       " '3r',\n",
       " 'Stargate_Continuum',\n",
       " 'Size/MD5_checksum_########',\n",
       " '■_Quail',\n",
       " 'Kong_Meng',\n",
       " 'Crazy_Glue',\n",
       " 'trainer_Buboy_Fernandez',\n",
       " 'Zone_EEZ',\n",
       " 'Canceled_warrants',\n",
       " 'Identical_quadruplets',\n",
       " 'ROCKVILLE_CENTRE',\n",
       " 'Thief_swipes',\n",
       " 'Bertossi',\n",
       " 'Viradouro_drum_queen',\n",
       " 'Mr_Lim_Chuan',\n",
       " 'testwork_carried',\n",
       " 'emperor_empress',\n",
       " 'posterior_ligaments',\n",
       " 'BioLucent',\n",
       " 'Kesarwani',\n",
       " 'anterior_vertebral',\n",
       " 'Color_Doppler',\n",
       " 'Nir_Barkat',\n",
       " 'Aerolineas',\n",
       " 'Danilo_Mangila',\n",
       " 'near_Witoka',\n",
       " 'coke_binges',\n",
       " 'FINMA',\n",
       " 'Lepidoptera',\n",
       " 'Homeric_proportions',\n",
       " 'Aladino_Pizza',\n",
       " 'PPAG',\n",
       " 'rootworm_beetles',\n",
       " 'Ruffin_McNeil',\n",
       " 'Yim_Pim',\n",
       " 'Experian_Hitwise',\n",
       " 'Avigen_beneficially_owned',\n",
       " 'simultaneous_Webcast',\n",
       " 'Danniels',\n",
       " 'Ryan_Idzi',\n",
       " 'Dubleo',\n",
       " 'gigabit_Gb',\n",
       " 'pacifists_protesting',\n",
       " 'Heatherwood',\n",
       " 'Anhydrous',\n",
       " 'COLUMBIA_Mo._Marcus_Denmon',\n",
       " 'Stock_Purchase_Agreements',\n",
       " 'townhouse',\n",
       " 'JAFZA',\n",
       " 'joyous_celebrations',\n",
       " 'Poverello_Center',\n",
       " 'FICO_Score',\n",
       " 'IITF',\n",
       " 'Nic_Bothma',\n",
       " 'Yolla_Bolly_Middle',\n",
       " 'Ek_haseena_thi',\n",
       " 'Fell_Asleep',\n",
       " 'Suspends_Dividend',\n",
       " 'contrat',\n",
       " 'Mazharul',\n",
       " 'bestiality_pedophilia',\n",
       " 'Michael_Ptasznik',\n",
       " 'Time_Punks',\n",
       " 'Halem',\n",
       " 'L._Balderston',\n",
       " 'roedd_y',\n",
       " 'Unstable_angina',\n",
       " 'rocket_boosters',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(word_emb_models[\"word2vec\"].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec\n",
      "Best parameters set found on development set:  {'C': 1000}\n",
      "Best F1 on development set: 0.85\n",
      "F1 on test set: 0.85\n",
      "time: 314.68\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec\")\n",
    "_, dense = run_pipeline(dataset, word_emb_models[\"word2vec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Both Word2Vec and GloVe are generic word embeddings. They were trained on general corpora which are not specific on any domains. Word2vec was trained on Google News Corpus. GloVe was trained on CommonCrawl Corpus.\n",
    "\n",
    "From the experiment, we can see that\n",
    "- tokenizer: unlike one hot vector representation, lowercase and lemmatization do not help much, and that makes sense! Recall the reason why lemmatization is useful for one hot vector? In that case, lemmatization can group words like \"good\" and \"best\" together to \"good\", to reduce the sparsity of vector space resulting in model being more certain to classify when see word \"good\". However, this is not the case for word embeddings. The pretrained models have vector for words like \"good\", \"better\" and \"best\" and those vector are similar enough to represent the idea of these words (positive sentiment), but still be able to capture subtle differences (best > better > good). Thus, it's better for word embeddings to leave these words as their original form\n",
    "- tfidf: TODO TODO (tfidf makes thing worse? why? did i do something wrong?)\n",
    "- polling function: From the experiment, although log function performs slighly better, the differences are not significant. The log function should work well for long document because it reduces the effect of words with more occurrence (Manning). One assumption we can make is that the IMDB review are not long enough (270 tokens) to observe the effect of using log function. Sum and Norm give the similar wor information retrieval tasks or any other tasks that require cosine similarity. But this is not the case for logistic regression, as shown in the experiment. Note that when we do normalization, the constants we apply for each training instances are their magniture so they are different among other. Also note that normalization in this context is different from feature normalization.\n",
    "\n",
    "\n",
    "Ones should expect that word embeddings (dense representation) should achieve higher performance than one hot vector (sparse representation). However, the experiment show that the best F1 achieved by word embeddings is about 0.86 (row 20). These are assumptions \n",
    "- OOV\n",
    "- Biased to train corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
