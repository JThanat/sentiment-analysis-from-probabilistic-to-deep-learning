{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nlp_utils import spacy_tokenizer_lower_lemma_remove_stop, preprocess_remove_html_non_ascii, spacy_tokenizer,spacy_tokenizer_remove_stop\n",
    "import spacy\n",
    "from dataset import download_tfds_imdb_as_text\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case, not sentenize\n",
    "uncase, not sentenize\n",
    "uncase, sentenize\n",
    "uncase, sentenize, replace punct, number, email, url\n",
    "\n",
    "dim\n",
    "\n",
    "300\n",
    "200\n",
    "100\n",
    "\n",
    "windows\n",
    "5\n",
    "15\n",
    "30\n",
    "50\n",
    "\n",
    "\n",
    "iter\n",
    "5,10,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples 25000\n",
      "number of testing samples 25000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = download_tfds_imdb_as_text()\n",
    "X_train_preprocessed = [preprocess_remove_html_non_ascii(doc) for doc in X_train]\n",
    "X_train_spacy = [doc for doc in nlp.pipe(X_train_preprocessed, n_threads=8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub(tok):\n",
    "    if tok.is_digit and tok.like_num:\n",
    "        return \"<<DIGIT>>\"\n",
    "    if tok.is_space:\n",
    "        return \" \"\n",
    "    if tok.like_url:\n",
    "        return \"<<URL>>\"\n",
    "    if tok.like_email:\n",
    "        return \"<<EMAIL>>\"\n",
    "    if tok.is_currency:\n",
    "        return \"<<$$>>\"\n",
    "    if tok.is_punct:\n",
    "        return \"<<PUNCT>>\"\n",
    "    \n",
    "    return tok.orth_\n",
    "\n",
    "\n",
    "\n",
    "case = [ [tok.orth_ for tok in doc] for doc in X_train_spacy]\n",
    "uncase = [ [tok.orth_.lower() for tok in doc] for doc in X_train_spacy]\n",
    "uncase_sent = [ [tok.orth_.lower() for tok in sent] for doc in X_train_spacy for sent in doc.sents]\n",
    "uncase_sent_sub = [ [sub(tok) for tok in sent] for doc in X_train_spacy for sent in doc.sents]\n",
    "\n",
    "corpus = {\"case\": case, \"uncase\": uncase, \"uncase_sent\":uncase_sent,\"uncase_sent_sub\":uncase_sent_sub}\n",
    "\n",
    "dims = [300, 200, 100]\n",
    "windows = [5,15,30,50]\n",
    "iters = [5,10,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_300_5_5\n",
      "case_300_5_10\n",
      "case_300_5_20\n",
      "case_300_15_5\n",
      "case_300_15_10\n",
      "case_300_15_20\n",
      "case_300_30_5\n",
      "case_300_30_10\n",
      "case_300_30_20\n",
      "case_300_50_5\n",
      "case_300_50_10\n",
      "case_300_50_20\n",
      "case_200_5_5\n",
      "case_200_5_10\n",
      "case_200_5_20\n",
      "case_200_15_5\n",
      "case_200_15_10\n",
      "case_200_15_20\n",
      "case_200_30_5\n",
      "case_200_30_10\n",
      "case_200_30_20\n",
      "case_200_50_5\n",
      "case_200_50_10\n",
      "case_200_50_20\n",
      "case_100_5_5\n",
      "case_100_5_10\n",
      "case_100_5_20\n",
      "case_100_15_5\n",
      "case_100_15_10\n",
      "case_100_15_20\n",
      "case_100_30_5\n",
      "case_100_30_10\n",
      "case_100_30_20\n",
      "case_100_50_5\n",
      "case_100_50_10\n",
      "case_100_50_20\n",
      "uncase_300_5_5\n",
      "uncase_300_5_10\n",
      "uncase_300_5_20\n",
      "uncase_300_15_5\n",
      "uncase_300_15_10\n",
      "uncase_300_15_20\n",
      "uncase_300_30_5\n",
      "uncase_300_30_10\n",
      "uncase_300_30_20\n",
      "uncase_300_50_5\n",
      "uncase_300_50_10\n",
      "uncase_300_50_20\n",
      "uncase_200_5_5\n",
      "uncase_200_5_10\n",
      "uncase_200_5_20\n",
      "uncase_200_15_5\n",
      "uncase_200_15_10\n",
      "uncase_200_15_20\n",
      "uncase_200_30_5\n",
      "uncase_200_30_10\n",
      "uncase_200_30_20\n",
      "uncase_200_50_5\n",
      "uncase_200_50_10\n",
      "uncase_200_50_20\n",
      "uncase_100_5_5\n",
      "uncase_100_5_10\n",
      "uncase_100_5_20\n",
      "uncase_100_15_5\n",
      "uncase_100_15_10\n",
      "uncase_100_15_20\n",
      "uncase_100_30_5\n",
      "uncase_100_30_10\n",
      "uncase_100_30_20\n",
      "uncase_100_50_5\n",
      "uncase_100_50_10\n",
      "uncase_100_50_20\n",
      "uncase_sent_300_5_5\n",
      "uncase_sent_300_5_10\n",
      "uncase_sent_300_5_20\n",
      "uncase_sent_300_15_5\n",
      "uncase_sent_300_15_10\n",
      "uncase_sent_300_15_20\n",
      "uncase_sent_300_30_5\n",
      "uncase_sent_300_30_10\n",
      "uncase_sent_300_30_20\n",
      "uncase_sent_300_50_5\n",
      "uncase_sent_300_50_10\n",
      "uncase_sent_300_50_20\n",
      "uncase_sent_200_5_5\n",
      "uncase_sent_200_5_10\n",
      "uncase_sent_200_5_20\n",
      "uncase_sent_200_15_5\n",
      "uncase_sent_200_15_10\n",
      "uncase_sent_200_15_20\n",
      "uncase_sent_200_30_5\n",
      "uncase_sent_200_30_10\n",
      "uncase_sent_200_30_20\n",
      "uncase_sent_200_50_5\n",
      "uncase_sent_200_50_10\n",
      "uncase_sent_200_50_20\n",
      "uncase_sent_100_5_5\n",
      "uncase_sent_100_5_10\n",
      "uncase_sent_100_5_20\n",
      "uncase_sent_100_15_5\n",
      "uncase_sent_100_15_10\n",
      "uncase_sent_100_15_20\n",
      "uncase_sent_100_30_5\n",
      "uncase_sent_100_30_10\n",
      "uncase_sent_100_30_20\n",
      "uncase_sent_100_50_5\n",
      "uncase_sent_100_50_10\n",
      "uncase_sent_100_50_20\n",
      "uncase_sent_sub_300_5_5\n",
      "uncase_sent_sub_300_5_10\n",
      "uncase_sent_sub_300_5_20\n",
      "uncase_sent_sub_300_15_5\n",
      "uncase_sent_sub_300_15_10\n",
      "uncase_sent_sub_300_15_20\n",
      "uncase_sent_sub_300_30_5\n",
      "uncase_sent_sub_300_30_10\n",
      "uncase_sent_sub_300_30_20\n",
      "uncase_sent_sub_300_50_5\n",
      "uncase_sent_sub_300_50_10\n",
      "uncase_sent_sub_300_50_20\n",
      "uncase_sent_sub_200_5_5\n",
      "uncase_sent_sub_200_5_10\n",
      "uncase_sent_sub_200_5_20\n",
      "uncase_sent_sub_200_15_5\n",
      "uncase_sent_sub_200_15_10\n",
      "uncase_sent_sub_200_15_20\n",
      "uncase_sent_sub_200_30_5\n",
      "uncase_sent_sub_200_30_10\n",
      "uncase_sent_sub_200_30_20\n",
      "uncase_sent_sub_200_50_5\n",
      "uncase_sent_sub_200_50_10\n",
      "uncase_sent_sub_200_50_20\n",
      "uncase_sent_sub_100_5_5\n",
      "uncase_sent_sub_100_5_10\n",
      "uncase_sent_sub_100_5_20\n",
      "uncase_sent_sub_100_15_5\n",
      "uncase_sent_sub_100_15_10\n",
      "uncase_sent_sub_100_15_20\n",
      "uncase_sent_sub_100_30_5\n",
      "uncase_sent_sub_100_30_10\n",
      "uncase_sent_sub_100_30_20\n",
      "uncase_sent_sub_100_50_5\n",
      "uncase_sent_sub_100_50_10\n",
      "uncase_sent_sub_100_50_20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for nc, c in corpus.items():\n",
    "    for d in dims:\n",
    "        for w in windows:\n",
    "            for ite in iters:\n",
    "                name = \"{}_{}_{}_{}\".format(nc, str(d), str(w), ite)\n",
    "                print(name)\n",
    "                model = Word2Vec(c, size=d, window=w, min_count=1, workers=4, sg=1, iter=ite)\n",
    "                model.save(name)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_emb import run_logistic_word_emb_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_emb_model</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>polling</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uncase_sent_300_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.867782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uncase_sent_300_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uncase_sent_300_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.870390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uncase_sent_300_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.875523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uncase_sent_300_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uncase_sent_300_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.870805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uncase_sent_300_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.879348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uncase_sent_300_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.874386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uncase_sent_300_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.871909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uncase_sent_300_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.877327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>uncase_sent_300_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>uncase_sent_300_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.872118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>uncase_sent_200_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.858762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uncase_sent_200_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.865566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>uncase_sent_200_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.866255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uncase_sent_200_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.874302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>uncase_sent_200_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.872785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>uncase_sent_200_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.868363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>uncase_sent_200_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.874301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>uncase_sent_200_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>uncase_sent_200_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.868946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>uncase_sent_200_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>uncase_sent_200_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.870917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>uncase_sent_200_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>uncase_sent_100_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.849741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>uncase_sent_100_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.855292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>uncase_sent_100_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.857419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>uncase_sent_100_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.864995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>uncase_sent_100_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.867230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>uncase_sent_100_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.862295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>uncase_sent_100_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.868635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>uncase_sent_100_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.865085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>uncase_sent_100_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.865369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>uncase_sent_100_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>uncase_sent_100_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.867276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>uncase_sent_100_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.861732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word_emb_model  tfidf              tokenizer polling        F1\n",
       "0     uncase_sent_300_5_5  False  spacy_tokenizer_lower     LOG  0.867782\n",
       "1    uncase_sent_300_5_10  False  spacy_tokenizer_lower     LOG  0.870100\n",
       "2    uncase_sent_300_5_20  False  spacy_tokenizer_lower     LOG  0.870390\n",
       "3    uncase_sent_300_15_5  False  spacy_tokenizer_lower     LOG  0.875523\n",
       "4   uncase_sent_300_15_10  False  spacy_tokenizer_lower     LOG  0.873086\n",
       "5   uncase_sent_300_15_20  False  spacy_tokenizer_lower     LOG  0.870805\n",
       "6    uncase_sent_300_30_5  False  spacy_tokenizer_lower     LOG  0.879348\n",
       "7   uncase_sent_300_30_10  False  spacy_tokenizer_lower     LOG  0.874386\n",
       "8   uncase_sent_300_30_20  False  spacy_tokenizer_lower     LOG  0.871909\n",
       "9    uncase_sent_300_50_5  False  spacy_tokenizer_lower     LOG  0.877327\n",
       "10  uncase_sent_300_50_10  False  spacy_tokenizer_lower     LOG  0.873641\n",
       "11  uncase_sent_300_50_20  False  spacy_tokenizer_lower     LOG  0.872118\n",
       "12    uncase_sent_200_5_5  False  spacy_tokenizer_lower     LOG  0.858762\n",
       "13   uncase_sent_200_5_10  False  spacy_tokenizer_lower     LOG  0.865566\n",
       "14   uncase_sent_200_5_20  False  spacy_tokenizer_lower     LOG  0.866255\n",
       "15   uncase_sent_200_15_5  False  spacy_tokenizer_lower     LOG  0.874302\n",
       "16  uncase_sent_200_15_10  False  spacy_tokenizer_lower     LOG  0.872785\n",
       "17  uncase_sent_200_15_20  False  spacy_tokenizer_lower     LOG  0.868363\n",
       "18   uncase_sent_200_30_5  False  spacy_tokenizer_lower     LOG  0.874301\n",
       "19  uncase_sent_200_30_10  False  spacy_tokenizer_lower     LOG  0.873867\n",
       "20  uncase_sent_200_30_20  False  spacy_tokenizer_lower     LOG  0.868946\n",
       "21   uncase_sent_200_50_5  False  spacy_tokenizer_lower     LOG  0.873418\n",
       "22  uncase_sent_200_50_10  False  spacy_tokenizer_lower     LOG  0.870917\n",
       "23  uncase_sent_200_50_20  False  spacy_tokenizer_lower     LOG  0.869730\n",
       "24    uncase_sent_100_5_5  False  spacy_tokenizer_lower     LOG  0.849741\n",
       "25   uncase_sent_100_5_10  False  spacy_tokenizer_lower     LOG  0.855292\n",
       "26   uncase_sent_100_5_20  False  spacy_tokenizer_lower     LOG  0.857419\n",
       "27   uncase_sent_100_15_5  False  spacy_tokenizer_lower     LOG  0.864995\n",
       "28  uncase_sent_100_15_10  False  spacy_tokenizer_lower     LOG  0.867230\n",
       "29  uncase_sent_100_15_20  False  spacy_tokenizer_lower     LOG  0.862295\n",
       "30   uncase_sent_100_30_5  False  spacy_tokenizer_lower     LOG  0.868635\n",
       "31  uncase_sent_100_30_10  False  spacy_tokenizer_lower     LOG  0.865085\n",
       "32  uncase_sent_100_30_20  False  spacy_tokenizer_lower     LOG  0.865369\n",
       "33   uncase_sent_100_50_5  False  spacy_tokenizer_lower     LOG  0.869045\n",
       "34  uncase_sent_100_50_10  False  spacy_tokenizer_lower     LOG  0.867276\n",
       "35  uncase_sent_100_50_20  False  spacy_tokenizer_lower     LOG  0.861732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "li=\"\"\"uncase_sent_300_5_5\n",
    "uncase_sent_300_5_10\n",
    "uncase_sent_300_5_20\n",
    "uncase_sent_300_15_5\n",
    "uncase_sent_300_15_10\n",
    "uncase_sent_300_15_20\n",
    "uncase_sent_300_30_5\n",
    "uncase_sent_300_30_10\n",
    "uncase_sent_300_30_20\n",
    "uncase_sent_300_50_5\n",
    "uncase_sent_300_50_10\n",
    "uncase_sent_300_50_20\n",
    "uncase_sent_200_5_5\n",
    "uncase_sent_200_5_10\n",
    "uncase_sent_200_5_20\n",
    "uncase_sent_200_15_5\n",
    "uncase_sent_200_15_10\n",
    "uncase_sent_200_15_20\n",
    "uncase_sent_200_30_5\n",
    "uncase_sent_200_30_10\n",
    "uncase_sent_200_30_20\n",
    "uncase_sent_200_50_5\n",
    "uncase_sent_200_50_10\n",
    "uncase_sent_200_50_20\n",
    "uncase_sent_100_5_5\n",
    "uncase_sent_100_5_10\n",
    "uncase_sent_100_5_20\n",
    "uncase_sent_100_15_5\n",
    "uncase_sent_100_15_10\n",
    "uncase_sent_100_15_20\n",
    "uncase_sent_100_30_5\n",
    "uncase_sent_100_30_10\n",
    "uncase_sent_100_30_20\n",
    "uncase_sent_100_50_5\n",
    "uncase_sent_100_50_10\n",
    "uncase_sent_100_50_20\"\"\".split('\\n')\n",
    "\n",
    "models = {e:Word2Vec.load(e) for e in li}\n",
    "\n",
    "\n",
    "run_logistic_word_emb_exp(X_train, X_test, y_train, y_test, models)\n",
    "\n",
    "\n",
    "# word2vec.model.240.10.10.filtered 88.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-446add96efa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m uncase_sent_sub_100_50_20\"\"\".split('\\n')\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-446add96efa2>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m uncase_sent_sub_100_50_20\"\"\".split('\\n')\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \"\"\"\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s recursively from %s.* with mmap=%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__numpys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from my_word2vec import MyWord2Vec\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_300_5_5\n",
      "(24764742, 102832)\n"
     ]
    }
   ],
   "source": [
    "for nc, c in list(corpus.items())[:1]:\n",
    "    for d in dims[:1]:\n",
    "        for w in windows[:1]:\n",
    "            for ite in iters[:1]:\n",
    "                name = \"{}_{}_{}_{}\".format(nc, str(d), str(w), ite)\n",
    "                print(name)\n",
    "                my_word2vec = MyWord2Vec(dim=300, window_size=2, epoch=5, batch_size=10, buffer_size=100, ignore=[\".\"])\n",
    "                vec, int2word = my_word2vec.train(c)\n",
    "                with open(name + \"_my_word2vec.pkl\", \"wb\") as f:\n",
    "                    pickle.dump((vec, int2word), f)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
