{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: From Naive Bayes to BERT \n",
    "\n",
    "**Introduction**\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "\n",
    "**Prerequisite**\n",
    "\n",
    "1. Install requirements \n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Download [Google Word2Vec Model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) to this directory (3M vocab, cased, 300 d) and run \n",
    "\n",
    "```\n",
    "gunzip GoogleNews-vectors-negative300.bin.gz\n",
    "```\n",
    "\n",
    "3. Download [Stanford GloVe Model](http://nlp.stanford.edu/data/glove.840B.300d.zip) (2.2M vocab, cased, 300d) to this directory and run the following commands.\n",
    "\n",
    "```\n",
    "unzip glove.840B.300d.zip\n",
    "python -m gensim.scripts.glove2word2vec --input  glove.840B.300d.txt --output glove.840B.300d.w2vformat.txt\n",
    "rm glove.840B.300d.(zip|txt)\n",
    "```\n",
    "\n",
    "Alternatively, GloVe can be used with SpaCy's `en_core_web_md` too. See [Document](https://spacy.io/models/en#en_core_web_md). In this notebook, we will not use GloVe from SpaCy due to lots of its limitations.\n",
    "\n",
    "4. Download Spacy model by running this command in terminal \n",
    "\n",
    "```\n",
    "python -m spacy download en_vectors_web_sm\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from dataset import download_tfds_imdb_as_text\n",
    "from classical_ml_models import run_logistic_exp, run_multi_nb_exp, run_ber_nb_exp\n",
    "from word_emb import run_logistic_word_emb_exp\n",
    "from nlp_utils import print_stat\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Prepare for experiment**\n",
    "\n",
    "- load word embeddings pretrained models\n",
    "- Download dataset and get to know it briefly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.utils_any2vec:duplicate word '����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������' in ./glove.840B.300d.w2vformat.txt, ignoring all but first\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples 25000\n",
      "number of testing samples 25000\n"
     ]
    }
   ],
   "source": [
    "word_emb_models = {\n",
    "    \"word2vec\": gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True),\n",
    "    \"glove\": gensim.models.KeyedVectors.load_word2vec_format('./glove.840B.300d.w2vformat.txt', binary=False) \n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = download_tfds_imdb_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of char 1325.07\n",
      "average number of tokens 272.43\n",
      "total number of vocab without stop words 120455\n",
      "most common: [('/><br', 50935), ('movie', 42423), ('film', 38841), ('like', 19414), ('good', 14327)]\n",
      "\n",
      "example\n",
      "Avoid this one, unless you want to watch an expensive but badly made movie. Example? The sound is good but the dialogue is not clear - a cardinal sin in a French film.<br /><br />This film attempts to combine western, drug intrigue and ancien regime costume epic. What? Well, consider this. The cowboy music is hilarious during sword fights. Or how about the woman in her underwear, holding a knife and jumping up and down on the bed?<br /><br />Someone should do a 'What's Up Tiger Lily' on this bomb. Rewrite the script and then either dub or subtitle it. Heck, it's almost that now. (BTW, Gerard Depardieu and Carole Bouquet, both known to American audiences, have roles.)\n",
      "\n",
      "This movie is a half-documentary...and it is pretty interesting....<br /><br />This is a good movie...based on the true story of how a bunch of norwegian saboturs managed to stop the heavy water production in Rukjan, Norway and the deliverance of the rest of the heavy water to Germany.<br /><br />This movie isn't perfect and it could have been a bit better... the best part of the movie is that some of the saboturs are played by themselves!!!<br /><br />If you're interested in history of WWII and film this is a movie that's worth a look!!\n"
     ]
    }
   ],
   "source": [
    "print_stat(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of char 1293.79\n",
      "average number of tokens 266.30\n",
      "total number of vocab without stop words 119407\n",
      "most common: [('/><br', 50039), ('movie', 42305), ('film', 38185), ('like', 19084), ('good', 13846)]\n",
      "\n",
      "example\n",
      "This movie was horrendous it was sorta like accidentally watching a gay porn waiting for the girls but they just don't come....I waited for almost 2 hours for the damn scarecrows....they just don't come...instead it's just some dumb ass wandering through a dead cornfield with a camera it's a mix of Blaire witch and some bad episode of the twilight zone. And the best part is that as of October 23 2005 they started filming a sequel please don't be fooled by the box even though it looks exactly the same as the first dark harvest it's not lions gate bought the rights to the Maize:the movie and had the brilliant idea to release it as the sequel to the original dark harvest;which i thought was funny........the only thing they had in common was they were both shot in a cornfield....This Movie WILLLLLL not scare the crop out of you like the first one so just stay away!!!!!\n",
      "\n",
      "Riggs and Murtough are back but the magic of the first film has disintegrated. The story line is just awful! I mean really, South African diplomats smuggling the mythical Krugerrands into the U.S. It's just painful! And the accents are absolutely abysmal! Can no one get an Afrikaans South African accent right? Or will we forever hear the British or Americans making them sound like drunken Hollanders? The only guy who got the Afrikaans accent right was Tim Robbins in Catch A Fire. Another thing about this movie that i disliked was when Danny Glover so artlessly describes an Afrikaans accent as being shitty! I mean what a slap in the face to the Afrikaans. There's also enough hypocrisy in this film to make me vomit. I mean Mel Gibson's character is like so against the diplomats but then sleeps with their P.A. type! Don't waste your time watching this rubbish non-researched film. If you want to see a film that doesn't completely insult a cultural group then rent Die Hard 2.\n"
     ]
    }
   ],
   "source": [
    "print_stat(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "At a glance, we know that\n",
    "- need to remove html tags during text preprocessing e.g. '/><br'\n",
    "- punctuation may be useful because it shows the excitement. \n",
    "- the average number of tokens is about 270. Keep this in mind when choosing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Probabilistic Models**\n",
    "\n",
    "The first experiment, we will use classical machine learning algorithms e.g. Logistic Regression and Naive Bayes. \n",
    "\n",
    "`X = IMDB_review`\n",
    "\n",
    "`y = 1 if the review is positive else 0`\n",
    "\n",
    "In order to create the vector X representing the IMDB reviews, we have to do the following process.\n",
    "\n",
    "**Tokenizer**\n",
    "\n",
    "I use SpaCy tokenizer with two different settings, see [documentation](https://spacy.io/usage/linguistic-features#tokenization) to understand its algorithm.\n",
    "\n",
    "1. SpaCy Tokenizer\n",
    "2. SpaCy Tokenizer + LowerCase + Lematization\n",
    "3. SpaCy Tokenizer + LowerCase + Lematization + Remove Stop Words\n",
    "\n",
    "The intuition of lowercase and lematization is that it can group words with similar meaning but in different form together. For example,\n",
    "\n",
    "`It is a good movie.`\n",
    "\n",
    "`It is the best movie.`\n",
    "\n",
    "If we tokenize and lemmatize these two sentneces, the results will share the token `good`. If we tokenize but not lemmatize, `good` and `best` will be different tokens.\n",
    "\n",
    "Lemmatization may or may not improve the accuracy of models. It depends on what kind of NLP tasks we are wokring on. Let's do the experiment and see how lemmatization effects the accuracy of our models and discuss why.\n",
    "\n",
    "Another thing we can do is to remove stop words. The words like `is`, `of` are likely to appear in almost every document so they provide very less information for the model to classify the document. Again, removing stop words might be useful or not useful. We can also say that the model like logistic regression can assign low weight to word features that carry less information. \n",
    "\n",
    "\n",
    "**Vectorization**\n",
    "There are several choices of text representations, see [documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)\n",
    "\n",
    "Here I use two types of vectorizer\n",
    "1. CountVectorizer\n",
    "2. TfidfVectorizer\n",
    "\n",
    "Somes setting we can play with is `min_df`, `max_df`, which are minimum and maxinum number of occurence of words across documents. Here I use the default setting, `min_df` is 1, `max_df` is not set.\n",
    "\n",
    "Note here that the intuition behind max_df is somewhat similar to remove stop words. The words like `is`, `I`, `of` are likely to appear in almost every document so it will be filted by `max_df`. These words are also filtered by stop words as well.\n",
    "\n",
    "Each vectorizer, I have four differnt configurations (2 x 2).\n",
    "1. Binary, Multinomial \n",
    "2. 1-gram, both 1-gram and 2-grams\n",
    "\n",
    "To understand the intuition behind n-grams, let's see this example.\n",
    "\n",
    "`This movie is not good. It is boring.`\n",
    "\n",
    "`This movie is not boring. It is good.` \n",
    "\n",
    "These two sentences have to exact same 1-gram but opposite sentiment. We have to use 2-grams to differentiate.\n",
    "\n",
    "\n",
    "**Models**\n",
    "1. Multinomial Naive Bayes, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
    "2. Bernuli Naive Bayes, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)\n",
    "3. Logistic Regression, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "The first two are generative model based on joint probability. They both estimate parameters using maxinum likelyhood. The different is how they define features. The third model are discriminative models. It estiamtes parameters using gradient descent. Learn more in [Manning's Information Retrieval](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf).\n",
    "\n",
    "Now let's start with the most classical model for sentiment analysis - Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>ngram</th>\n",
       "      <th>binary/multinomial</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.798752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.821933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.843808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.853251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.792631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.810695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.840261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.849568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.804507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.817771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.835297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.847843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vectorizer                     preprocessing  \\\n",
       "0   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "1   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "2   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "3   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "4   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "5   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "6   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "7   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "8   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "9   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "10  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "11  TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "\n",
       "                                  tokenizer   ngram binary/multinomial  \\\n",
       "0                           spacy_tokenizer  (1, 1)              False   \n",
       "1                           spacy_tokenizer  (1, 1)              False   \n",
       "2                           spacy_tokenizer  (1, 2)              False   \n",
       "3                           spacy_tokenizer  (1, 2)              False   \n",
       "4               spacy_tokenizer_lower_lemma  (1, 1)              False   \n",
       "5               spacy_tokenizer_lower_lemma  (1, 1)              False   \n",
       "6               spacy_tokenizer_lower_lemma  (1, 2)              False   \n",
       "7               spacy_tokenizer_lower_lemma  (1, 2)              False   \n",
       "8   spacy_tokenizer_lower_lemma_remove_stop  (1, 1)              False   \n",
       "9   spacy_tokenizer_lower_lemma_remove_stop  (1, 1)              False   \n",
       "10  spacy_tokenizer_lower_lemma_remove_stop  (1, 2)              False   \n",
       "11  spacy_tokenizer_lower_lemma_remove_stop  (1, 2)              False   \n",
       "\n",
       "          F1  \n",
       "0   0.798752  \n",
       "1   0.821933  \n",
       "2   0.843808  \n",
       "3   0.853251  \n",
       "4   0.792631  \n",
       "5   0.810695  \n",
       "6   0.840261  \n",
       "7   0.849568  \n",
       "8   0.804507  \n",
       "9   0.817771  \n",
       "10  0.835297  \n",
       "11  0.847843  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_multi_nb_exp(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>ngram</th>\n",
       "      <th>binary/multinomial</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.824464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.846779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.816929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.842736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.798459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.800929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        vectorizer                     preprocessing  \\\n",
       "0  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "1  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "2  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "3  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "4  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "5  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "\n",
       "                                 tokenizer   ngram binary/multinomial  \\\n",
       "0                          spacy_tokenizer  (1, 1)               True   \n",
       "1                          spacy_tokenizer  (1, 2)               True   \n",
       "2              spacy_tokenizer_lower_lemma  (1, 1)               True   \n",
       "3              spacy_tokenizer_lower_lemma  (1, 2)               True   \n",
       "4  spacy_tokenizer_lower_lemma_remove_stop  (1, 1)               True   \n",
       "5  spacy_tokenizer_lower_lemma_remove_stop  (1, 2)               True   \n",
       "\n",
       "         F1  \n",
       "0  0.824464  \n",
       "1  0.846779  \n",
       "2  0.816929  \n",
       "3  0.842736  \n",
       "4  0.798459  \n",
       "5  0.800929  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ber_nb_exp(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the logistic regression experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/pataveemeemeng/jupyter-projects/sentiment-analysis-from-probabilistic-to-deep-learning/venv/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>ngram</th>\n",
       "      <th>binary/multinomial</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.887099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.891876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.885628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.884538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.901420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.909439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.899214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.902088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.880550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.887447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.880778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.882487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.903955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.911705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.901219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>preprocess_remove_html_non_ascii</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.905520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vectorizer                     preprocessing  \\\n",
       "0   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "1   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "2   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "3   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "4   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "5   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "6   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "7   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "8   CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "9   TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "10  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "11  TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "12  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "13  TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "14  CountVectorizer  preprocess_remove_html_non_ascii   \n",
       "15  TfidfVectorizer  preprocess_remove_html_non_ascii   \n",
       "\n",
       "                      tokenizer   ngram binary/multinomial        F1  \n",
       "0               spacy_tokenizer  (1, 1)               True  0.887099  \n",
       "1               spacy_tokenizer  (1, 1)               True  0.891876  \n",
       "2               spacy_tokenizer  (1, 1)              False  0.885628  \n",
       "3               spacy_tokenizer  (1, 1)              False  0.884538  \n",
       "4               spacy_tokenizer  (1, 2)               True  0.901420  \n",
       "5               spacy_tokenizer  (1, 2)               True  0.909439  \n",
       "6               spacy_tokenizer  (1, 2)              False  0.899214  \n",
       "7               spacy_tokenizer  (1, 2)              False  0.902088  \n",
       "8   spacy_tokenizer_lower_lemma  (1, 1)               True  0.880550  \n",
       "9   spacy_tokenizer_lower_lemma  (1, 1)               True  0.887447  \n",
       "10  spacy_tokenizer_lower_lemma  (1, 1)              False  0.880778  \n",
       "11  spacy_tokenizer_lower_lemma  (1, 1)              False  0.882487  \n",
       "12  spacy_tokenizer_lower_lemma  (1, 2)               True  0.903955  \n",
       "13  spacy_tokenizer_lower_lemma  (1, 2)               True  0.911705  \n",
       "14  spacy_tokenizer_lower_lemma  (1, 2)              False  0.901219  \n",
       "15  spacy_tokenizer_lower_lemma  (1, 2)              False  0.905520  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logistic_exp(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "**Word Embeddings**\n",
    "\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_emb_model</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>polling</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.822629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.819563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.832477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.840142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.827647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.824966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.845811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.845131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.856787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.853586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.852953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.850226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.823411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.822611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.832649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.841725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.829774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.826332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.849639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.845893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.859760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.857544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.854826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.851301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.819213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.817248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.829618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.837943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.827381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.823809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.845591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.841647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.855746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.854112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.850492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.847639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_emb_model  tfidf                                tokenizer polling  \\\n",
       "0        word2vec   True  spacy_tokenizer_lower_lemma_remove_stop    norm   \n",
       "1           glove   True  spacy_tokenizer_lower_lemma_remove_stop    norm   \n",
       "2        word2vec   True                          spacy_tokenizer    norm   \n",
       "3           glove   True                          spacy_tokenizer    norm   \n",
       "4        word2vec   True              spacy_tokenizer_lower_lemma    norm   \n",
       "5           glove   True              spacy_tokenizer_lower_lemma    norm   \n",
       "6        word2vec  False  spacy_tokenizer_lower_lemma_remove_stop    norm   \n",
       "7           glove  False  spacy_tokenizer_lower_lemma_remove_stop    norm   \n",
       "8        word2vec  False                          spacy_tokenizer    norm   \n",
       "9           glove  False                          spacy_tokenizer    norm   \n",
       "10       word2vec  False              spacy_tokenizer_lower_lemma    norm   \n",
       "11          glove  False              spacy_tokenizer_lower_lemma    norm   \n",
       "12       word2vec   True  spacy_tokenizer_lower_lemma_remove_stop     LOG   \n",
       "13          glove   True  spacy_tokenizer_lower_lemma_remove_stop     LOG   \n",
       "14       word2vec   True                          spacy_tokenizer     LOG   \n",
       "15          glove   True                          spacy_tokenizer     LOG   \n",
       "16       word2vec   True              spacy_tokenizer_lower_lemma     LOG   \n",
       "17          glove   True              spacy_tokenizer_lower_lemma     LOG   \n",
       "18       word2vec  False  spacy_tokenizer_lower_lemma_remove_stop     LOG   \n",
       "19          glove  False  spacy_tokenizer_lower_lemma_remove_stop     LOG   \n",
       "20       word2vec  False                          spacy_tokenizer     LOG   \n",
       "21          glove  False                          spacy_tokenizer     LOG   \n",
       "22       word2vec  False              spacy_tokenizer_lower_lemma     LOG   \n",
       "23          glove  False              spacy_tokenizer_lower_lemma     LOG   \n",
       "24       word2vec   True  spacy_tokenizer_lower_lemma_remove_stop     SUM   \n",
       "25          glove   True  spacy_tokenizer_lower_lemma_remove_stop     SUM   \n",
       "26       word2vec   True                          spacy_tokenizer     SUM   \n",
       "27          glove   True                          spacy_tokenizer     SUM   \n",
       "28       word2vec   True              spacy_tokenizer_lower_lemma     SUM   \n",
       "29          glove   True              spacy_tokenizer_lower_lemma     SUM   \n",
       "30       word2vec  False  spacy_tokenizer_lower_lemma_remove_stop     SUM   \n",
       "31          glove  False  spacy_tokenizer_lower_lemma_remove_stop     SUM   \n",
       "32       word2vec  False                          spacy_tokenizer     SUM   \n",
       "33          glove  False                          spacy_tokenizer     SUM   \n",
       "34       word2vec  False              spacy_tokenizer_lower_lemma     SUM   \n",
       "35          glove  False              spacy_tokenizer_lower_lemma     SUM   \n",
       "\n",
       "          F1  \n",
       "0   0.822629  \n",
       "1   0.819563  \n",
       "2   0.832477  \n",
       "3   0.840142  \n",
       "4   0.827647  \n",
       "5   0.824966  \n",
       "6   0.845811  \n",
       "7   0.845131  \n",
       "8   0.856787  \n",
       "9   0.853586  \n",
       "10  0.852953  \n",
       "11  0.850226  \n",
       "12  0.823411  \n",
       "13  0.822611  \n",
       "14  0.832649  \n",
       "15  0.841725  \n",
       "16  0.829774  \n",
       "17  0.826332  \n",
       "18  0.849639  \n",
       "19  0.845893  \n",
       "20  0.859760  \n",
       "21  0.857544  \n",
       "22  0.854826  \n",
       "23  0.851301  \n",
       "24  0.819213  \n",
       "25  0.817248  \n",
       "26  0.829618  \n",
       "27  0.837943  \n",
       "28  0.827381  \n",
       "29  0.823809  \n",
       "30  0.845591  \n",
       "31  0.841647  \n",
       "32  0.855746  \n",
       "33  0.854112  \n",
       "34  0.850492  \n",
       "35  0.847639  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logistic_word_emb_exp(X_train, X_test, y_train, y_test, word_emb_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
