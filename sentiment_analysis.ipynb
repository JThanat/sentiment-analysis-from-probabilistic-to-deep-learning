{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: From Naive Bayes to BERT \n",
    "\n",
    "**Introduction**\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "TODO TODO TODO\n",
    "\n",
    "\n",
    "**Prerequisite**\n",
    "\n",
    "1. Install requirements \n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Download [Google Word2Vec Model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) to this directory (3M vocab, cased, 300 d) and run \n",
    "\n",
    "```\n",
    "gunzip GoogleNews-vectors-negative300.bin.gz\n",
    "```\n",
    "\n",
    "3. Download [Stanford GloVe Model](http://nlp.stanford.edu/data/glove.840B.300d.zip) (2.2M vocab, cased, 300d) to this directory and run the following commands.\n",
    "\n",
    "```\n",
    "unzip glove.840B.300d.zip\n",
    "python -m gensim.scripts.glove2word2vec --input  glove.840B.300d.txt --output glove.840B.300d.w2vformat.txt\n",
    "```\n",
    "\n",
    "Alternatively, GloVe can be used with SpaCy's `en_core_web_md` too. See [Document](https://spacy.io/models/en#en_core_web_md). In this notebook, we will not use GloVe from SpaCy due to lots of its limitations.\n",
    "\n",
    "4. Download Spacy model by running this command in terminal \n",
    "\n",
    "```\n",
    "python -m spacy download en_vectors_web_sm\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classical_ml_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0426a74f7008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_tfds_imdb_as_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclassical_ml_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_logistic_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_multi_nb_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_ber_nb_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mword_emb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_logistic_word_emb_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnlp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_stat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'classical_ml_models'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from dataset import download_tfds_imdb_as_text\n",
    "from classical_ml_models import run_logistic_exp, run_multi_nb_exp, run_ber_nb_exp\n",
    "from word_emb import run_logistic_word_emb_exp\n",
    "from nlp_utils import print_stat\n",
    "\n",
    "import gensim\n",
    "\n",
    "# There are warnings about optimization not converge within max iter. I am aware of that but to make the\n",
    "# output pretty, just disable it for now.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Prepare for experiment**\n",
    "\n",
    "- load word embeddings pretrained models\n",
    "- Download dataset and get to know it briefly. (GTKY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples 25000\n",
      "number of testing samples 25000\n"
     ]
    }
   ],
   "source": [
    "word_emb_models = {\n",
    "    \"word2vec\": gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True),\n",
    "    \"glove\": gensim.models.KeyedVectors.load_word2vec_format('./glove.840B.300d.w2vformat.txt', binary=False) \n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = download_tfds_imdb_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of char 1325.07\n",
      "average number of tokens 272.43\n",
      "total number of vocab without stop words 120455\n",
      "most common: [('/><br', 50935), ('movie', 42423), ('film', 38841), ('like', 19414), ('good', 14327)]\n",
      "word embedding model: word2vec, num oov: 41715, percent of oov: 0.35\n",
      "word embedding model: glove, num oov: 35440, percent of oov: 0.29\n",
      "\n",
      "example\n",
      "Avoid this one, unless you want to watch an expensive but badly made movie. Example? The sound is good but the dialogue is not clear - a cardinal sin in a French film.<br /><br />This film attempts to combine western, drug intrigue and ancien regime costume epic. What? Well, consider this. The cowboy music is hilarious during sword fights. Or how about the woman in her underwear, holding a knife and jumping up and down on the bed?<br /><br />Someone should do a 'What's Up Tiger Lily' on this bomb. Rewrite the script and then either dub or subtitle it. Heck, it's almost that now. (BTW, Gerard Depardieu and Carole Bouquet, both known to American audiences, have roles.)\n",
      "\n",
      "This movie is a half-documentary...and it is pretty interesting....<br /><br />This is a good movie...based on the true story of how a bunch of norwegian saboturs managed to stop the heavy water production in Rukjan, Norway and the deliverance of the rest of the heavy water to Germany.<br /><br />This movie isn't perfect and it could have been a bit better... the best part of the movie is that some of the saboturs are played by themselves!!!<br /><br />If you're interested in history of WWII and film this is a movie that's worth a look!!\n"
     ]
    }
   ],
   "source": [
    "print_stat(X_train, word_emb_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of char 1293.79\n",
      "average number of tokens 266.30\n",
      "total number of vocab without stop words 119407\n",
      "most common: [('/><br', 50039), ('movie', 42305), ('film', 38185), ('like', 19084), ('good', 13846)]\n",
      "word embedding model: word2vec, num oov: 41108, percent of oov: 0.34\n",
      "word embedding model: glove, num oov: 34966, percent of oov: 0.29\n",
      "\n",
      "example\n",
      "This movie was horrendous it was sorta like accidentally watching a gay porn waiting for the girls but they just don't come....I waited for almost 2 hours for the damn scarecrows....they just don't come...instead it's just some dumb ass wandering through a dead cornfield with a camera it's a mix of Blaire witch and some bad episode of the twilight zone. And the best part is that as of October 23 2005 they started filming a sequel please don't be fooled by the box even though it looks exactly the same as the first dark harvest it's not lions gate bought the rights to the Maize:the movie and had the brilliant idea to release it as the sequel to the original dark harvest;which i thought was funny........the only thing they had in common was they were both shot in a cornfield....This Movie WILLLLLL not scare the crop out of you like the first one so just stay away!!!!!\n",
      "\n",
      "Riggs and Murtough are back but the magic of the first film has disintegrated. The story line is just awful! I mean really, South African diplomats smuggling the mythical Krugerrands into the U.S. It's just painful! And the accents are absolutely abysmal! Can no one get an Afrikaans South African accent right? Or will we forever hear the British or Americans making them sound like drunken Hollanders? The only guy who got the Afrikaans accent right was Tim Robbins in Catch A Fire. Another thing about this movie that i disliked was when Danny Glover so artlessly describes an Afrikaans accent as being shitty! I mean what a slap in the face to the Afrikaans. There's also enough hypocrisy in this film to make me vomit. I mean Mel Gibson's character is like so against the diplomats but then sleeps with their P.A. type! Don't waste your time watching this rubbish non-researched film. If you want to see a film that doesn't completely insult a cultural group then rent Die Hard 2.\n"
     ]
    }
   ],
   "source": [
    "print_stat(X_test, word_emb_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "At a glance, we know that\n",
    "- need to remove html tags during text preprocessing e.g. '/><br'\n",
    "- punctuation may be useful because it shows the excitement. \n",
    "- the average number of tokens is about 270. Keep this in mind when choosing models\n",
    "- number of out-of-vocab (oov), which is the number of words that are not in word embeddings. It's 34% for word2vec and 29% for GloVe. Keep these numbers in mind, we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Probabilistic Models**\n",
    "\n",
    "The first experiment, we will use classical machine learning algorithms e.g. Logistic Regression and Naive Bayes. \n",
    "\n",
    "`X = IMDB_review`\n",
    "\n",
    "`y = 1 if the review is positive else 0`\n",
    "\n",
    "In order to create the vector X representing the IMDB reviews, we have to do the following process.\n",
    "\n",
    "**Tokenizer**\n",
    "\n",
    "I use SpaCy tokenizer with two different settings, see [documentation](https://spacy.io/usage/linguistic-features#tokenization) to understand its algorithm.\n",
    "\n",
    "1. SpaCy Tokenizer\n",
    "2. SpaCy Tokenizer + LowerCase + Lematization\n",
    "3. SpaCy Tokenizer + LowerCase + Lematization + Remove Stop Words\n",
    "\n",
    "The intuition of lowercase and lematization is that it can group words with similar meaning but in different form together. For example,\n",
    "\n",
    "`It is a good movie.`\n",
    "\n",
    "`It is the best movie.`\n",
    "\n",
    "If we tokenize and lemmatize these two sentneces, the results will share the token `good`. If we tokenize but not lemmatize, `good` and `best` will be different tokens.\n",
    "\n",
    "Lemmatization may or may not improve the accuracy of models. It depends on what kind of NLP tasks we are wokring on. Let's do the experiment and see how lemmatization effects the accuracy of our models and discuss why.\n",
    "\n",
    "Another thing we can do is to remove stop words. The words like `is`, `of` are likely to appear in almost every document so they provide very less information for the model to classify the document. Again, removing stop words might be useful or not useful. We can also say that the model like logistic regression can assign low weight to word features that carry less information. \n",
    "\n",
    "\n",
    "**Vectorization**\n",
    "There are several choices of text representations, see [documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)\n",
    "\n",
    "Here I use two types of vectorizer\n",
    "1. CountVectorizer\n",
    "2. TfidfVectorizer\n",
    "\n",
    "Somes setting we can play with is `min_df`, `max_df`, which are minimum and maxinum number of occurence of words across documents. Here I use the default setting, `min_df` is 1, `max_df` is not set.\n",
    "\n",
    "Note here that the intuition behind max_df is somewhat similar to remove stop words. The words like `is`, `I`, `of` are likely to appear in almost every document so it will be filted by `max_df`. These words are also filtered by stop words as well.\n",
    "\n",
    "Each vectorizer, I have four differnt configurations (2 x 2).\n",
    "1. Binary, Multinomial \n",
    "2. 1-gram, both 1-gram and 2-grams\n",
    "\n",
    "To understand the intuition behind n-grams, let's see this example.\n",
    "\n",
    "`This movie is not good. It is boring.`\n",
    "\n",
    "`This movie is not boring. It is good.` \n",
    "\n",
    "These two sentences have to exact same 1-gram but opposite sentiment. We have to use 2-grams to differentiate.\n",
    "\n",
    "\n",
    "**Models**\n",
    "1. Multinomial Naive Bayes, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
    "2. Bernuli Naive Bayes, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)\n",
    "3. Logistic Regression, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "The first two are generative model based on joint probability. They both estimate parameters using maxinum likelyhood. The different is how they define features. The third model are discriminative models. It estiamtes parameters using gradient descent. Learn more in [Manning's Information Retrieval](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf).\n",
    "\n",
    "Now let's start with the most classical model for sentiment analysis - Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>ngram</th>\n",
       "      <th>binary/multinomial</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.798752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.821933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.843808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.853251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.792631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.810695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.840261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.849568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.804507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.817771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.835297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.847843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vectorizer                                tokenizer   ngram  \\\n",
       "0   CountVectorizer                          spacy_tokenizer  (1, 1)   \n",
       "1   TfidfVectorizer                          spacy_tokenizer  (1, 1)   \n",
       "2   CountVectorizer                          spacy_tokenizer  (1, 2)   \n",
       "3   TfidfVectorizer                          spacy_tokenizer  (1, 2)   \n",
       "4   CountVectorizer              spacy_tokenizer_lower_lemma  (1, 1)   \n",
       "5   TfidfVectorizer              spacy_tokenizer_lower_lemma  (1, 1)   \n",
       "6   CountVectorizer              spacy_tokenizer_lower_lemma  (1, 2)   \n",
       "7   TfidfVectorizer              spacy_tokenizer_lower_lemma  (1, 2)   \n",
       "8   CountVectorizer  spacy_tokenizer_lower_lemma_remove_stop  (1, 1)   \n",
       "9   TfidfVectorizer  spacy_tokenizer_lower_lemma_remove_stop  (1, 1)   \n",
       "10  CountVectorizer  spacy_tokenizer_lower_lemma_remove_stop  (1, 2)   \n",
       "11  TfidfVectorizer  spacy_tokenizer_lower_lemma_remove_stop  (1, 2)   \n",
       "\n",
       "   binary/multinomial        F1  \n",
       "0               False  0.798752  \n",
       "1               False  0.821933  \n",
       "2               False  0.843808  \n",
       "3               False  0.853251  \n",
       "4               False  0.792631  \n",
       "5               False  0.810695  \n",
       "6               False  0.840261  \n",
       "7               False  0.849568  \n",
       "8               False  0.804507  \n",
       "9               False  0.817771  \n",
       "10              False  0.835297  \n",
       "11              False  0.847843  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_multi_nb_exp(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>ngram</th>\n",
       "      <th>binary/multinomial</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.824464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.846779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.816929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.842736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.798459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma_remove_stop</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.800929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        vectorizer                                tokenizer   ngram  \\\n",
       "0  CountVectorizer                          spacy_tokenizer  (1, 1)   \n",
       "1  CountVectorizer                          spacy_tokenizer  (1, 2)   \n",
       "2  CountVectorizer              spacy_tokenizer_lower_lemma  (1, 1)   \n",
       "3  CountVectorizer              spacy_tokenizer_lower_lemma  (1, 2)   \n",
       "4  CountVectorizer  spacy_tokenizer_lower_lemma_remove_stop  (1, 1)   \n",
       "5  CountVectorizer  spacy_tokenizer_lower_lemma_remove_stop  (1, 2)   \n",
       "\n",
       "  binary/multinomial        F1  \n",
       "0               True  0.824464  \n",
       "1               True  0.846779  \n",
       "2               True  0.816929  \n",
       "3               True  0.842736  \n",
       "4               True  0.798459  \n",
       "5               True  0.800929  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ber_nb_exp(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the logistic regression experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>ngram</th>\n",
       "      <th>binary/multinomial</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.887099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.891876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.885628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.884538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.901420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.909439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.899214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.902088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.880550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.887447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.880778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.882487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.903955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.911705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.901219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.905520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vectorizer                    tokenizer   ngram binary/multinomial  \\\n",
       "0   CountVectorizer              spacy_tokenizer  (1, 1)               True   \n",
       "1   TfidfVectorizer              spacy_tokenizer  (1, 1)               True   \n",
       "2   CountVectorizer              spacy_tokenizer  (1, 1)              False   \n",
       "3   TfidfVectorizer              spacy_tokenizer  (1, 1)              False   \n",
       "4   CountVectorizer              spacy_tokenizer  (1, 2)               True   \n",
       "5   TfidfVectorizer              spacy_tokenizer  (1, 2)               True   \n",
       "6   CountVectorizer              spacy_tokenizer  (1, 2)              False   \n",
       "7   TfidfVectorizer              spacy_tokenizer  (1, 2)              False   \n",
       "8   CountVectorizer  spacy_tokenizer_lower_lemma  (1, 1)               True   \n",
       "9   TfidfVectorizer  spacy_tokenizer_lower_lemma  (1, 1)               True   \n",
       "10  CountVectorizer  spacy_tokenizer_lower_lemma  (1, 1)              False   \n",
       "11  TfidfVectorizer  spacy_tokenizer_lower_lemma  (1, 1)              False   \n",
       "12  CountVectorizer  spacy_tokenizer_lower_lemma  (1, 2)               True   \n",
       "13  TfidfVectorizer  spacy_tokenizer_lower_lemma  (1, 2)               True   \n",
       "14  CountVectorizer  spacy_tokenizer_lower_lemma  (1, 2)              False   \n",
       "15  TfidfVectorizer  spacy_tokenizer_lower_lemma  (1, 2)              False   \n",
       "\n",
       "          F1  \n",
       "0   0.887099  \n",
       "1   0.891876  \n",
       "2   0.885628  \n",
       "3   0.884538  \n",
       "4   0.901420  \n",
       "5   0.909439  \n",
       "6   0.899214  \n",
       "7   0.902088  \n",
       "8   0.880550  \n",
       "9   0.887447  \n",
       "10  0.880778  \n",
       "11  0.882487  \n",
       "12  0.903955  \n",
       "13  0.911705  \n",
       "14  0.901219  \n",
       "15  0.905520  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logistic_exp(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "**3. Word Embeddings**\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "\n",
    "Dense Representation -> Jurafsky Book\n",
    "\n",
    "WordVec -> http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ (easy), hard -> original paper\n",
    "GloVe \n",
    "\n",
    "\n",
    "**Pretrained Word Embeddings**\n",
    "\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_emb_model</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>polling</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.832477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.840142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.827647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.824966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.856787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.853586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.852953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.850226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.832649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.841725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.829774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.826332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.859760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.857544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.854826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.851301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.829618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.837943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.827381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>glove</td>\n",
       "      <td>True</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.823809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.855746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.854112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.850492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>glove</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_lemma</td>\n",
       "      <td>SUM</td>\n",
       "      <td>0.847639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_emb_model  tfidf                    tokenizer polling        F1\n",
       "0        word2vec   True              spacy_tokenizer    norm  0.832477\n",
       "1           glove   True              spacy_tokenizer    norm  0.840142\n",
       "2        word2vec   True  spacy_tokenizer_lower_lemma    norm  0.827647\n",
       "3           glove   True  spacy_tokenizer_lower_lemma    norm  0.824966\n",
       "4        word2vec  False              spacy_tokenizer    norm  0.856787\n",
       "5           glove  False              spacy_tokenizer    norm  0.853586\n",
       "6        word2vec  False  spacy_tokenizer_lower_lemma    norm  0.852953\n",
       "7           glove  False  spacy_tokenizer_lower_lemma    norm  0.850226\n",
       "8        word2vec   True              spacy_tokenizer     LOG  0.832649\n",
       "9           glove   True              spacy_tokenizer     LOG  0.841725\n",
       "10       word2vec   True  spacy_tokenizer_lower_lemma     LOG  0.829774\n",
       "11          glove   True  spacy_tokenizer_lower_lemma     LOG  0.826332\n",
       "12       word2vec  False              spacy_tokenizer     LOG  0.859760\n",
       "13          glove  False              spacy_tokenizer     LOG  0.857544\n",
       "14       word2vec  False  spacy_tokenizer_lower_lemma     LOG  0.854826\n",
       "15          glove  False  spacy_tokenizer_lower_lemma     LOG  0.851301\n",
       "16       word2vec   True              spacy_tokenizer     SUM  0.829618\n",
       "17          glove   True              spacy_tokenizer     SUM  0.837943\n",
       "18       word2vec   True  spacy_tokenizer_lower_lemma     SUM  0.827381\n",
       "19          glove   True  spacy_tokenizer_lower_lemma     SUM  0.823809\n",
       "20       word2vec  False              spacy_tokenizer     SUM  0.855746\n",
       "21          glove  False              spacy_tokenizer     SUM  0.854112\n",
       "22       word2vec  False  spacy_tokenizer_lower_lemma     SUM  0.850492\n",
       "23          glove  False  spacy_tokenizer_lower_lemma     SUM  0.847639"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logistic_word_emb_exp(X_train, X_test, y_train, y_test, word_emb_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Both Word2Vec and GloVe are generic word embeddings. They were trained on general corpora which are not specific on any domains. Word2vec was trained on Google News Corpus. GloVe was trained on CommonCrawl Corpus.\n",
    "\n",
    "From the experiment, we can see that\n",
    "- tokenizer: unlike one hot vector representation, lowercase and lemmatization do not help much, and that makes sense! Recall the reason why lemmatization is useful for one hot vector? In that case, lemmatization can group words like \"good\" and \"best\" together to \"good\", to reduce the sparsity of vector space resulting in model being more certain to classify when see word \"good\". However, this is not the case for word embeddings. The pretrained models have vector for words like \"good\", \"better\" and \"best\" and those vector are similar enough to represent the idea of these words (positive sentiment), but still be able to capture subtle differences (best > better > good). Thus, it's better for word embeddings to leave these words as their original form\n",
    "- tfidf: TODO TODO (tfidf makes thing worse? why? did i do something wrong?)\n",
    "- polling function: From the experiment, although log function performs slighly better, the differences are not significant. The log function should work well for long document because it reduces the effect of words with more occurrence (Manning). One assumption we can make is that the IMDB review are not long enough (270 tokens) to observe the effect of using log function. Sum and Norm give the similar wor information retrieval tasks or any other tasks that require cosine similarity. But this is not the case for logistic regression, as shown in the experiment. Note that when we do normalization, the constants we apply for each training instances are their magniture so they are different among other. Also note that normalization in this context is different from feature normalization.\n",
    "\n",
    "\n",
    "Ones should expect that word embeddings (dense representation) should achieve higher performance than one hot vector (sparse representation). However, the experiment show that the best F1 achieved by word embeddings is about 0.86 (row 20). These are assumptions \n",
    "- OOV\n",
    "- Biased to train corpora\n",
    "\n",
    "In order to prove these assumptions;\n",
    "\n",
    "**Train Word Embeddings From Scratch**\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "\n",
    "\n",
    "**Train Word Embeddings - Start from Pre-trained (Transfer Learning)**\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_emb_model</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>polling</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uncase_300_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uncase_300_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uncase_300_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uncase_300_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.880992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uncase_300_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.880158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uncase_300_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.877717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uncase_300_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.883318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uncase_300_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.880117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uncase_300_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.879858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uncase_300_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.881192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>uncase_300_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.879650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>uncase_300_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.877116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>uncase_200_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.865608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uncase_200_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>uncase_200_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.870597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uncase_200_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.880942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>uncase_200_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.875812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>uncase_200_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>uncase_200_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.881496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>uncase_200_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.878037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>uncase_200_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.876695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>uncase_200_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>uncase_200_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.877098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>uncase_200_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.875754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>uncase_100_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.855136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>uncase_100_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.860692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>uncase_100_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.859493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>uncase_100_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.874007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>uncase_100_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>uncase_100_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.871378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>uncase_100_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.875287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>uncase_100_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.874114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>uncase_100_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.870325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>uncase_100_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>uncase_100_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>uncase_100_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.868599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_emb_model  tfidf              tokenizer polling        F1\n",
       "0     uncase_300_5_5  False  spacy_tokenizer_lower     LOG  0.869426\n",
       "1    uncase_300_5_10  False  spacy_tokenizer_lower     LOG  0.873276\n",
       "2    uncase_300_5_20  False  spacy_tokenizer_lower     LOG  0.873170\n",
       "3    uncase_300_15_5  False  spacy_tokenizer_lower     LOG  0.880992\n",
       "4   uncase_300_15_10  False  spacy_tokenizer_lower     LOG  0.880158\n",
       "5   uncase_300_15_20  False  spacy_tokenizer_lower     LOG  0.877717\n",
       "6    uncase_300_30_5  False  spacy_tokenizer_lower     LOG  0.883318\n",
       "7   uncase_300_30_10  False  spacy_tokenizer_lower     LOG  0.880117\n",
       "8   uncase_300_30_20  False  spacy_tokenizer_lower     LOG  0.879858\n",
       "9    uncase_300_50_5  False  spacy_tokenizer_lower     LOG  0.881192\n",
       "10  uncase_300_50_10  False  spacy_tokenizer_lower     LOG  0.879650\n",
       "11  uncase_300_50_20  False  spacy_tokenizer_lower     LOG  0.877116\n",
       "12    uncase_200_5_5  False  spacy_tokenizer_lower     LOG  0.865608\n",
       "13   uncase_200_5_10  False  spacy_tokenizer_lower     LOG  0.869897\n",
       "14   uncase_200_5_20  False  spacy_tokenizer_lower     LOG  0.870597\n",
       "15   uncase_200_15_5  False  spacy_tokenizer_lower     LOG  0.880942\n",
       "16  uncase_200_15_10  False  spacy_tokenizer_lower     LOG  0.875812\n",
       "17  uncase_200_15_20  False  spacy_tokenizer_lower     LOG  0.873911\n",
       "18   uncase_200_30_5  False  spacy_tokenizer_lower     LOG  0.881496\n",
       "19  uncase_200_30_10  False  spacy_tokenizer_lower     LOG  0.878037\n",
       "20  uncase_200_30_20  False  spacy_tokenizer_lower     LOG  0.876695\n",
       "21   uncase_200_50_5  False  spacy_tokenizer_lower     LOG  0.880000\n",
       "22  uncase_200_50_10  False  spacy_tokenizer_lower     LOG  0.877098\n",
       "23  uncase_200_50_20  False  spacy_tokenizer_lower     LOG  0.875754\n",
       "24    uncase_100_5_5  False  spacy_tokenizer_lower     LOG  0.855136\n",
       "25   uncase_100_5_10  False  spacy_tokenizer_lower     LOG  0.860692\n",
       "26   uncase_100_5_20  False  spacy_tokenizer_lower     LOG  0.859493\n",
       "27   uncase_100_15_5  False  spacy_tokenizer_lower     LOG  0.874007\n",
       "28  uncase_100_15_10  False  spacy_tokenizer_lower     LOG  0.873941\n",
       "29  uncase_100_15_20  False  spacy_tokenizer_lower     LOG  0.871378\n",
       "30   uncase_100_30_5  False  spacy_tokenizer_lower     LOG  0.875287\n",
       "31  uncase_100_30_10  False  spacy_tokenizer_lower     LOG  0.874114\n",
       "32  uncase_100_30_20  False  spacy_tokenizer_lower     LOG  0.870325\n",
       "33   uncase_100_50_5  False  spacy_tokenizer_lower     LOG  0.873363\n",
       "34  uncase_100_50_10  False  spacy_tokenizer_lower     LOG  0.869607\n",
       "35  uncase_100_50_20  False  spacy_tokenizer_lower     LOG  0.868599"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "li=\"\"\"uncase_300_5_5\n",
    "uncase_300_5_10\n",
    "uncase_300_5_20\n",
    "uncase_300_15_5\n",
    "uncase_300_15_10\n",
    "uncase_300_15_20\n",
    "uncase_300_30_5\n",
    "uncase_300_30_10\n",
    "uncase_300_30_20\n",
    "uncase_300_50_5\n",
    "uncase_300_50_10\n",
    "uncase_300_50_20\n",
    "uncase_200_5_5\n",
    "uncase_200_5_10\n",
    "uncase_200_5_20\n",
    "uncase_200_15_5\n",
    "uncase_200_15_10\n",
    "uncase_200_15_20\n",
    "uncase_200_30_5\n",
    "uncase_200_30_10\n",
    "uncase_200_30_20\n",
    "uncase_200_50_5\n",
    "uncase_200_50_10\n",
    "uncase_200_50_20\n",
    "uncase_100_5_5\n",
    "uncase_100_5_10\n",
    "uncase_100_5_20\n",
    "uncase_100_15_5\n",
    "uncase_100_15_10\n",
    "uncase_100_15_20\n",
    "uncase_100_30_5\n",
    "uncase_100_30_10\n",
    "uncase_100_30_20\n",
    "uncase_100_50_5\n",
    "uncase_100_50_10\n",
    "uncase_100_50_20\"\"\".split('\\n')\n",
    "\n",
    "models = {e:Word2Vec.load(e) for e in li}\n",
    "\n",
    "\n",
    "run_logistic_word_emb_exp(X_train, X_test, y_train, y_test, models)\n",
    "\n",
    "\n",
    "# word2vec.model.240.10.10.filtered 88.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Sequential Model**\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fa3c26a4e80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Transformer**\n",
    "\n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO \n",
    "\n",
    "TODO TODO TODO TODO TODO TODO TODO TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_emb_model</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>polling</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uncase_sent_sub_300_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.862712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uncase_sent_sub_300_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.866216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uncase_sent_sub_300_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.868402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uncase_sent_sub_300_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.875205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uncase_sent_sub_300_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uncase_sent_sub_300_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.871489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uncase_sent_sub_300_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.874573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uncase_sent_sub_300_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.872355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uncase_sent_sub_300_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.870823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uncase_sent_sub_300_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.875211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>uncase_sent_sub_300_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>uncase_sent_sub_300_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>uncase_sent_sub_200_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.856373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uncase_sent_sub_200_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.862004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>uncase_sent_sub_200_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.864390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uncase_sent_sub_200_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>uncase_sent_sub_200_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>uncase_sent_sub_200_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>uncase_sent_sub_200_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.873226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>uncase_sent_sub_200_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.869439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>uncase_sent_sub_200_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.868322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>uncase_sent_sub_200_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.871445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>uncase_sent_sub_200_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.872449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>uncase_sent_sub_200_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.868107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>uncase_sent_sub_100_5_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.846064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>uncase_sent_sub_100_5_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.850635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>uncase_sent_sub_100_5_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.851986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>uncase_sent_sub_100_15_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.861273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>uncase_sent_sub_100_15_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.863506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>uncase_sent_sub_100_15_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.860632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>uncase_sent_sub_100_30_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.865987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>uncase_sent_sub_100_30_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.863504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>uncase_sent_sub_100_30_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.860683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>uncase_sent_sub_100_50_5</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.864676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>uncase_sent_sub_100_50_10</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.865748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>uncase_sent_sub_100_50_20</td>\n",
       "      <td>False</td>\n",
       "      <td>spacy_tokenizer_lower_sub</td>\n",
       "      <td>LOG</td>\n",
       "      <td>0.860458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word_emb_model  tfidf                  tokenizer polling  \\\n",
       "0     uncase_sent_sub_300_5_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "1    uncase_sent_sub_300_5_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "2    uncase_sent_sub_300_5_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "3    uncase_sent_sub_300_15_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "4   uncase_sent_sub_300_15_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "5   uncase_sent_sub_300_15_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "6    uncase_sent_sub_300_30_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "7   uncase_sent_sub_300_30_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "8   uncase_sent_sub_300_30_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "9    uncase_sent_sub_300_50_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "10  uncase_sent_sub_300_50_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "11  uncase_sent_sub_300_50_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "12    uncase_sent_sub_200_5_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "13   uncase_sent_sub_200_5_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "14   uncase_sent_sub_200_5_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "15   uncase_sent_sub_200_15_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "16  uncase_sent_sub_200_15_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "17  uncase_sent_sub_200_15_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "18   uncase_sent_sub_200_30_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "19  uncase_sent_sub_200_30_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "20  uncase_sent_sub_200_30_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "21   uncase_sent_sub_200_50_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "22  uncase_sent_sub_200_50_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "23  uncase_sent_sub_200_50_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "24    uncase_sent_sub_100_5_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "25   uncase_sent_sub_100_5_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "26   uncase_sent_sub_100_5_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "27   uncase_sent_sub_100_15_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "28  uncase_sent_sub_100_15_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "29  uncase_sent_sub_100_15_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "30   uncase_sent_sub_100_30_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "31  uncase_sent_sub_100_30_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "32  uncase_sent_sub_100_30_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "33   uncase_sent_sub_100_50_5  False  spacy_tokenizer_lower_sub     LOG   \n",
       "34  uncase_sent_sub_100_50_10  False  spacy_tokenizer_lower_sub     LOG   \n",
       "35  uncase_sent_sub_100_50_20  False  spacy_tokenizer_lower_sub     LOG   \n",
       "\n",
       "          F1  \n",
       "0   0.862712  \n",
       "1   0.866216  \n",
       "2   0.868402  \n",
       "3   0.875205  \n",
       "4   0.873131  \n",
       "5   0.871489  \n",
       "6   0.874573  \n",
       "7   0.872355  \n",
       "8   0.870823  \n",
       "9   0.875211  \n",
       "10  0.873072  \n",
       "11  0.869537  \n",
       "12  0.856373  \n",
       "13  0.862004  \n",
       "14  0.864390  \n",
       "15  0.869701  \n",
       "16  0.869429  \n",
       "17  0.869797  \n",
       "18  0.873226  \n",
       "19  0.869439  \n",
       "20  0.868322  \n",
       "21  0.871445  \n",
       "22  0.872449  \n",
       "23  0.868107  \n",
       "24  0.846064  \n",
       "25  0.850635  \n",
       "26  0.851986  \n",
       "27  0.861273  \n",
       "28  0.863506  \n",
       "29  0.860632  \n",
       "30  0.865987  \n",
       "31  0.863504  \n",
       "32  0.860683  \n",
       "33  0.864676  \n",
       "34  0.865748  \n",
       "35  0.860458  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "li=\"\"\"uncase_sent_sub_300_5_5\n",
    "uncase_sent_sub_300_5_10\n",
    "uncase_sent_sub_300_5_20\n",
    "uncase_sent_sub_300_15_5\n",
    "uncase_sent_sub_300_15_10\n",
    "uncase_sent_sub_300_15_20\n",
    "uncase_sent_sub_300_30_5\n",
    "uncase_sent_sub_300_30_10\n",
    "uncase_sent_sub_300_30_20\n",
    "uncase_sent_sub_300_50_5\n",
    "uncase_sent_sub_300_50_10\n",
    "uncase_sent_sub_300_50_20\n",
    "uncase_sent_sub_200_5_5\n",
    "uncase_sent_sub_200_5_10\n",
    "uncase_sent_sub_200_5_20\n",
    "uncase_sent_sub_200_15_5\n",
    "uncase_sent_sub_200_15_10\n",
    "uncase_sent_sub_200_15_20\n",
    "uncase_sent_sub_200_30_5\n",
    "uncase_sent_sub_200_30_10\n",
    "uncase_sent_sub_200_30_20\n",
    "uncase_sent_sub_200_50_5\n",
    "uncase_sent_sub_200_50_10\n",
    "uncase_sent_sub_200_50_20\n",
    "uncase_sent_sub_100_5_5\n",
    "uncase_sent_sub_100_5_10\n",
    "uncase_sent_sub_100_5_20\n",
    "uncase_sent_sub_100_15_5\n",
    "uncase_sent_sub_100_15_10\n",
    "uncase_sent_sub_100_15_20\n",
    "uncase_sent_sub_100_30_5\n",
    "uncase_sent_sub_100_30_10\n",
    "uncase_sent_sub_100_30_20\n",
    "uncase_sent_sub_100_50_5\n",
    "uncase_sent_sub_100_50_10\n",
    "uncase_sent_sub_100_50_20\"\"\".split('\\n')\n",
    "\n",
    "models = {e:Word2Vec.load(e) for e in li}\n",
    "\n",
    "\n",
    "run_logistic_word_emb_exp(X_train, X_test, y_train, y_test, models)\n",
    "\n",
    "\n",
    "# word2vec.model.240.10.10.filtered 88.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
