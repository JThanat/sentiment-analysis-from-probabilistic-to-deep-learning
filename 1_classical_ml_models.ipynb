{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from lib.dataset import download_tfds_imdb_as_text, download_tfds_imdb_as_text_tiny\n",
    "from lib.classical_ml import run_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = download_tfds_imdb_as_text()\n",
    "tiny_dataset = download_tfds_imdb_as_text_tiny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple SpaCy tokenizer\n",
      "Load tokenized document from disk\n",
      "Load tokenized document from disk\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'is_logistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-34f9df3b9785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mexp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-34f9df3b9785>\u001b[0m in \u001b[0;36mexp1\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Simple SpaCy tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Simple SpaCy tokenizer and ignore stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/classical_ml.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(dataset, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# train model and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_logistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     pipe = Pipeline(\n",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/classical_ml.py\u001b[0m in \u001b[0;36mcreate_model_obj\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_logistic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         model = LogisticRegression(\n\u001b[1;32m     42\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_logistic' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def exp1(dataset):\n",
    "    \n",
    "    print(\"Simple SpaCy tokenizer\")\n",
    "    _, _ = run_pipeline(dataset)\n",
    "\n",
    "    print(\"Simple SpaCy tokenizer and ignore stop\")\n",
    "    _, _ = run_pipeline(dataset, lower=True, ignore=[\"like_num\", \"is_stop\"])\n",
    "    \n",
    "    print(\"Simple SpaCy tokenizer, lowercase, lemma\")\n",
    "    _, _ = run_pipeline(dataset, lower=True, lemma=True)\n",
    "    \n",
    "    \n",
    "\n",
    "# approximate running time: 16 mins\n",
    "import time\n",
    "now = time.time()\n",
    "exp1(dataset)\n",
    "print(time.time()-now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 Vectorizer\n",
    "\n",
    "Prerequisite: If you are not familiar with TFIDF, read see [this](https://nlp.stanford.edu/IR-book/pdf/06vect.pdf).\n",
    "\n",
    "In this experiement, we will try different vectorization techniques; Bigrams, TFIDF and Binary. Although all of them are based one hot encoding, they capture slightly different information from text. \n",
    "\n",
    "- Bigrams:\n",
    "\n",
    "- TFIDF: TFIDF is very common technique for Information Retrieval (IR) and has been long proved that it improves the IR performance. However, text classification and IR are two different problem sets, so this is not neccessary the case. For text classification, we can expect that whatever classification models we use can capture the same things as TFIDF, which is how importance words are, or how much information words contain.\n",
    "\n",
    "- Binary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def exp2(dataset):\n",
    "    \n",
    "    print(\"Simple SpaCy tokenizer\")\n",
    "    _, _ = run_pipeline(dataset, lower=True, lemma=True, bigram=True)\n",
    "\n",
    "    print(\"Simple SpaCy tokenizer and ignore stop\")\n",
    "    _, _ = run_pipeline(dataset, lower=True, lemma=True, tfidf=True)\n",
    "      \n",
    "    print(\"Simple SpaCy tokenizer, lowercase, lemma\")\n",
    "    _, _ = run_pipeline(dataset, lower=True, lemma=True, binary=True)\n",
    "    \n",
    "    \n",
    "\n",
    "# approximate running time: 16 mins\n",
    "import time\n",
    "now = time.time()\n",
    "exp2(dataset)\n",
    "print(time.time()-now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 Model\n",
    "\n",
    "- Naive Bayes\n",
    "- Logisitc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
